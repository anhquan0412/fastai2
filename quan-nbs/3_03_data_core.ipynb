{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.torch_basics import *\n",
    "from fastai2.data.load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data core\n",
    "\n",
    "> Core functionality for gathering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes here provide functionality for applying a list of transforms to a set of items (`TfmdList`, `DataSource`) or a `DataLoader` (`TfmdDl`) as well as the base class used to gather the data for model training: `DataBunch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfmdDL -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x, y, samples, ctxs=None, max_n=9, **kwargs):\n",
    "    if ctxs is None: ctxs = Inf.nones\n",
    "    for i in range_of(samples[0]):\n",
    "        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- show_batch is a type-dispatched function that is responsible for showing decoded samples. \n",
    "- x and y are the input and the target in the batch to be shown, and are passed along to dispatch on their types. \n",
    "    - There is a different implementation of show_batch if x is a TensorImage or a TensorText for instance (see vision.core or text.data for more details). \n",
    "- ctxs can be passed but the function is responsible to create them if necessary. \n",
    "- kwargs depend on the specific implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x, y, samples, outs, ctxs=None, max_n=9, **kwargs):\n",
    "    if ctxs is None: ctxs = Inf.nones\n",
    "    for i in range(len(samples[0])):\n",
    "        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n",
    "    for i in range(len(outs[0])):\n",
    "        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(i),ctxs,range(max_n))]\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_all_ = [\"show_batch\", \"show_results\"]\n",
    "_batch_tfms = ('after_item','before_batch','after_batch') # list specific callbacks. EACH OF THEM HAS THEIR OWN PIPELINE, aka their own encode/decode\n",
    "# 'after_item': (after grabbing a single item in dataset/ where before_batch pipeline is used) \n",
    "    # transform each item in tuple (where as_item=False).\n",
    "    # example of after_item tfms: item_img_tfms = [ImageResizer(128), ToTensor()]\n",
    "# 'after_batch': run AFTER tuples being collated together by Pytorch dataloader in a batch.\n",
    "    #This transform run on a whole batch at a time \n",
    "    # GPU transformations go here\n",
    "# 'before_batch': after getting all items together, but right BEFORE collating them into batch \n",
    "    #(the only one that will be done as a whole thing at a time (as_item=True). The rest will be done as Tuple Transform, \n",
    "    # aka perform transformation for each value in the tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates() #Note: this decorator will replace the **kwargs with argument names of its parent class (DataLoader) when you shift tab, thus helps with autocompletion\n",
    "class TfmdDL(DataLoader): # fastai DataLoader that UNDERSTAND TRANSFORM\n",
    "    \"Transformed `DataLoader`\"\n",
    "    def __init__(self, dataset, bs=64, shuffle=False, num_workers=None, verbose=False, do_setup=True, **kwargs):\n",
    "        if num_workers is None: num_workers = min(16, defaults.cpus)\n",
    "        for nm in _batch_tfms: \n",
    "            kwargs[nm] = Pipeline(kwargs.get(nm,None), as_item=(nm=='before_batch')) \n",
    "            # Turn after_item, before_batch ... into pipeline here, and set only 'before_batch' as item transform\n",
    "        super().__init__(dataset, bs=bs, shuffle=shuffle, num_workers=num_workers, **kwargs)\n",
    "        if do_setup: # do setup for pipeline here. See pipeline setup in 6_04_transform.ipynb\n",
    "            for nm in _batch_tfms: \n",
    "                pv(f\"Setting up {nm}: {kwargs[nm]}\", verbose)\n",
    "                kwargs[nm].setup(self)\n",
    "\n",
    "    def _one_pass(self):\n",
    "        b = self.do_batch([self.do_item(0)])\n",
    "        if self.device is not None: b = to_device(b, self.device)\n",
    "        its = self.after_batch(b)\n",
    "        self._n_inp = 1 if not isinstance(its, (list,tuple)) or len(its)==1 else len(its)-1\n",
    "        self._types = mapped(type,its)\n",
    "        \n",
    "        \n",
    "    # to retain the original type of data (secret sauce for type reserve) at the end of the batch      \n",
    "    def _retain_dl(self,b):\n",
    "        if not getattr(self, '_types', None): self._one_pass()\n",
    "        return retain_types(b, typs=self._types)\n",
    "\n",
    "    @delegates(DataLoader.new)\n",
    "    def new(self, dataset=None, cls=None, **kwargs):\n",
    "        res = super().new(dataset, cls, do_setup=False, **kwargs)\n",
    "        if not hasattr(self, '_n_inp') or not hasattr(self, '_types'):\n",
    "            try: \n",
    "                self._one_pass()\n",
    "                res._n_inp,res._types = self._n_inp,self._types\n",
    "            except: print(\"Could not do one pass in your dataloader, there is something wrong in it\")\n",
    "        else: res._n_inp,res._types = self._n_inp,self._types\n",
    "        return res\n",
    "\n",
    "    def before_iter(self):\n",
    "        super().before_iter()\n",
    "        split_idx = getattr(self.dataset, 'split_idx', None)\n",
    "        for nm in _batch_tfms:\n",
    "            f = getattr(self,nm)\n",
    "            if isinstance(f,Pipeline): f.split_idx=split_idx\n",
    "\n",
    "\n",
    "    def decode(self, b): return self.before_batch.decode(to_cpu(self.after_batch.decode(self._retain_dl(b)))) #put back the type for type reserve\n",
    "    def decode_batch(self, b, max_n=9, full=True): return self._decode_batch(self.decode(b), max_n, full)\n",
    "    def _decode_batch(self, b, max_n=9, full=True):\n",
    "        f = self.after_item.decode\n",
    "        # good old compose. This compose is a function though (see below)\n",
    "        f = compose(f, partial(getattr(self.dataset,'decode',noop), full = full))        \n",
    "        return L(batch_to_samples(b, max_n=max_n)).map(f)\n",
    "\n",
    "    def _pre_show_batch(self, b, max_n=9):\n",
    "        \"Decode `b` to be ready for `show_batch`\"\n",
    "        b = self.decode(b)\n",
    "        if hasattr(b, 'show'): return b,None,None\n",
    "        its = self._decode_batch(b, max_n, full=False)\n",
    "        if not is_listy(b): b,its = [b],L((o,) for o in its)\n",
    "        return detuplify(b[:self.n_inp]),detuplify(b[self.n_inp:]),its\n",
    "        \n",
    "    def show_batch(self, b=None, max_n=9, ctxs=None, **kwargs):\n",
    "        \"Show `b` (defaults to `one_batch`), a list of lists of pipeline outputs (i.e. output of a `DataLoader`)\"\n",
    "        '''\n",
    "        1. pass in some batch to show\n",
    "        2. decode that batch (which includes put back the type): use after batch and before batch transform: before_batch.decode and after_batch.decode\n",
    "        3. context: can be from matplotlib (axes) or pandas dataframe. Can be fetched from the type of the obj in the batch\n",
    "        4. Call .show()\n",
    "        '''\n",
    "        if b is None: b = self.one_batch()\n",
    "        if not show: return self._pre_show_batch(b, max_n=max_n)\n",
    "        show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "    \n",
    "    def show_results(self, b, out, max_n=9, ctxs=None, show=True, **kwargs):\n",
    "        x,y,its = self.show_batch(b, max_n=max_n, show=False)\n",
    "        b_out = b[:self.n_inp] + (tuple(out) if is_listy(out) else (out,))\n",
    "        x1,y1,outs = self.show_batch(b_out, max_n=max_n, show=False)\n",
    "        res = (x,x1,None,None) if its is None else (x, y, its, outs.itemgot(slice(self.n_inp,None)))\n",
    "        #its == None means that a batch knows how to show itself as a whole, so we pass x, x1\n",
    "        if not show: return res\n",
    "        show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "            \n",
    "#     @property\n",
    "#     def device(self):\n",
    "#         if not getattr(self, '_device', None): self._one_pass()\n",
    "#         return self._device\n",
    "    \n",
    "#     @device.setter\n",
    "#     def device(self, v): self._device = v\n",
    "\n",
    "    @property\n",
    "    def n_inp(self):\n",
    "        if hasattr(self.dataset, 'n_inp'): return self.dataset.n_inp\n",
    "        if not hasattr(self, '_n_inp'): self._one_pass()\n",
    "        return self._n_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def compose(*funcs, order=None):\n",
    "    \"Create a function that composes (do) all functions in `funcs`, passing along remaining `*args` and `**kwargs` to all\"\n",
    "    funcs = L(funcs)\n",
    "    if order is not None: funcs = funcs.sorted(order)\n",
    "    def _inner(x, *args, **kwargs):\n",
    "        for f in L(funcs): x = f(x, *args, **kwargs)\n",
    "        return x\n",
    "    return _inner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TfmdDL(\n",
    "    dataset,\n",
    "    bs=16,\n",
    "    shuffle=False,\n",
    "    num_workers=None,\n",
    "    drop_last=False,\n",
    "    indexed=None,\n",
    "    pin_memory=False,\n",
    "    timeout=0,\n",
    "    *,\n",
    "    wif=None,\n",
    "    before_iter=None,\n",
    "    create_batches=None,\n",
    "    sampler=None,\n",
    "    create_item=None,\n",
    "    after_item=None,\n",
    "    before_batch=None,\n",
    "    create_batch=None,\n",
    "    retain=None,\n",
    "    after_batch=None,\n",
    "    after_iter=None,\n",
    "    get_idxs=None,\n",
    ")\n",
    "\n",
    "DataLoader(\n",
    "    dataset=None,\n",
    "    bs=None,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    indexed=None,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    timeout=0,\n",
    "    *,\n",
    "    wif=None,\n",
    "    before_iter=None,\n",
    "    create_batches=None,\n",
    "    sampler=None,\n",
    "    create_item=None,\n",
    "    after_item=None,\n",
    "    before_batch=None,\n",
    "    create_batch=None,\n",
    "    retain=None,\n",
    "    after_batch=None,\n",
    "    after_iter=None,\n",
    "    get_idxs=None,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TfmdDL` is a `DataLoader` that creates `Pipeline` from a list of `Transform`s for the callbacks `after_item`, `before_batch` and `after_batch`. As a result, it can decode or show a processed `batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(TfmdDL,\n",
    "         decode=\"Decode `b` using `tfms`\",\n",
    "         decode_batch=\"Decode `b` entirely\",\n",
    "         new=\"Create a new version of self with a few changed attributes\",\n",
    "         show_batch=\"Show `b` (defaults to `one_batch`), a list of lists of pipeline outputs (i.e. output of a `DataLoader`)\",\n",
    "         show_results=\"Show each item of `b` and `out`\",\n",
    "         before_iter=\"override\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Category(int, ShowTitle): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type reserve (part 2, continue from 2_05 notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(TensorImage([1]),),\n",
       " (TensorImage([1]),),\n",
       " (TensorImage([1]),),\n",
       " (TensorImage([1]),)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(TensorImage([1]),)] * 4\n",
    "# why a list of \"tuple of 1\" instead of just [TensorImage([1]),TensorImage([1]), ...]?\n",
    "# since this list is supposed to be from Datasets (below), and it's always a tuple because\n",
    "# Datasets have multiple lists (normally 2, 1 for X 1 for Y)\n",
    "# simpler output of Datasets print(dsets) => [(1,),(0,),(0,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test retain type\n",
    "class NegTfm(Transform):\n",
    "    def encodes(self, x): return torch.neg(x)\n",
    "    def decodes(self, x): return torch.neg(x)\n",
    "    \n",
    "tdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=NegTfm(), bs=4, num_workers=4)\n",
    "b = tdl.one_batch()\n",
    "test_eq(type(b[0]), TensorImage)\n",
    "# even though NegTfn.encode will return a torch.Tensor (bc of torch.neg), which is the pytorch type (determined by .type())\n",
    "    # the normal type (type that is determined by type(<obj>) is still reserved (fastai TensorImage)\n",
    "# Note: this only works if the output is the parent class of the input (TensorImage is the child of torch.Tensor)\n",
    "\n",
    "#See more in cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai2.torch_core.TensorImage'> torch.LongTensor\n",
      "<class 'fastai2.torch_core.TensorImage'> torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "temp = TensorImage([1])\n",
    "print(type(temp),temp.type()) #input\n",
    "print(type(b[0]), b[0].type()) #output\n",
    "\n",
    "# so both the pytorch type and normal type of these 2 are the same, \n",
    "# probably since there is no IntToFloat tensor transformation. If there is, then pytorch type will change (look at 2_05_data_transforms notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai2.torch_core.TensorImage'> torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "print(type(tdl.decode_batch(b)[0][0]), (tdl.decode_batch(b)[0][0]).type()) #decode the output\n",
    "test_eq(type(tdl.decode_batch(b)[0][0]), TensorImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.FloatTensor\n",
      "<class 'fastai2.torch_core.TensorImage'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# Note: Because ALL OF TRANSFORMATION PIPELINE (TfmDL) ALL CHECK (AFTER GOING THROUGH ENCODE/DECODE OF EACH TFMS) that TYPE MUST BE RESERVED\n",
    "b = (tensor([1.,1.,1.,1.]),)\n",
    "print(type(b[0]),b[0].type()) #before decode\n",
    "print(type(tdl.decode_batch(b)[0][0]),(tdl.decode_batch(b)[0][0]).type()) #after decode\n",
    "\n",
    "# even when you try to decode b which has different normal type, \n",
    "# tdl (TfmdDL with specified TensorImage as input) decoding will convert b to TensorImage type, hence 'type reserved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force no type reserve with ->None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegTfm(Transform):\n",
    "    def encodes(self, x)->None: return torch.neg(x) # the '-> None' means not enforcing type consistency\n",
    "    def decodes(self, x): return torch.neg(x)\n",
    "    \n",
    "tdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=NegTfm(), bs=4, num_workers=4)\n",
    "b = tdl.one_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(type(b[0]), TensorImage) # failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai2.torch_core.TensorImage'> torch.LongTensor\n",
      "<class 'torch.Tensor'> torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "temp = TensorImage([1])\n",
    "print(type(temp),temp.type()) #input\n",
    "print(type(b[0]), b[0].type()) #output. Don't reserve type. Type will be strictly depends on encode function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: This examples below is inconsistent, because changing A to no type reserve, or change f(x) to type reserve, they all end up with same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(Transform): \n",
    "    def encodes(self, x): return x \n",
    "    def decodes(self, x): return Int(x) \n",
    "\n",
    "@Transform\n",
    "def f(x)->None: return Tuple((x,x)) # not enforcing 'reserve input type' setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.arange(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = A()\n",
    "tdl = TfmdDL(start, after_item=lambda x: (a(x), f(x)), bs=4) \n",
    "# input: 1 single item from a, torch.Tensor type (Look at _batch_tfms create_item below for more info)\n",
    "\n",
    "# return two things: a(x) which is itself and f(x) which is Tuple type\n",
    "x,y = tdl.one_batch()\n",
    "test_eq(type(y), Tuple) # encode forward? type Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, 'torch.LongTensor')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(start[0]), start[0].type() #input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.LongTensor\n",
      "<class 'fastcore.utils.Tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x), x.type()) #output x\n",
    "print(type(y)) #output y. \n",
    "# Note that since f(x) doesn't reserve type, y normal type (Tuple) isn't converted to torch.Tensor type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]),\n",
       " (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3])),\n",
       " fastcore.utils.Tuple)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y,type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [(tensor(0), (tensor(0), tensor(0))),(tensor(1), (tensor(1), tensor(1))),(tensor(2), (tensor(2), tensor(2))),(tensor(3), (tensor(3), tensor(3)))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = tdl.decode_batch((x,y)) # since bs = 4, return L list of 4\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), (tensor(0), tensor(0)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "print(type(s[0][0]),s[0][0].type()) #decode for x, which is result of A transform\n",
    "# => with type reserve, both types are exactly the same with start's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastcore.utils.Tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(s[0][1])) # decode for y, which is f(x) transform with no type reserve. \n",
    "# => original normal type is kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = TfmdDL(torch.arange(0,50), after_item=A(), after_batch=NegTfm(), bs=4)\n",
    "test_eq(tdl.dataset[0], start[0])\n",
    "test_eq(len(tdl), (50-1)//4+1)\n",
    "test_eq(tdl.bs, 4)\n",
    "test_stdout(tdl.show_batch, '0\\n1\\n2\\n3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders (formerly known as DataBunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??GetAttr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why does GetAttr exist:\n",
    "- 1st: because the default getattr will get EVERYTHING\n",
    "\n",
    "Potential problem: get s.t with hidden error. For example a typo db.on_batch() instead of db.one_batch(), this typo function definitely be called using normal \\__getattr__ instead of getting handled properly\n",
    "\n",
    "- 2nd: standard \\__getattr__ has no tab completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def add_props(f, n=2):\n",
    "    \"Create properties passing each of `range(n)` to f\"\n",
    "    return (property(partial(f,i)) for i in range(n))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "# what's going on inside add_props\n",
    "tempf1 = lambda i,x: x[i]\n",
    "tempf2 = [partial(tempf1,i) for i in range(2)] # just an array of 2 functions, one with default param i=0 and one with default param i=1\n",
    "print(tempf2[0](['a','b']))# first function (with i=0) got called with params ['a','b']\n",
    "print(tempf2[1](['a','b']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old: databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@docs\n",
    "class DataBunch(GetAttr): \n",
    "    # GetAttr: wrapper around __getattr__, using '_default' to set what object will be used in default\n",
    "    \"Basic wrapper around several `DataLoader`s.\"\n",
    "    _default='train_dl' # default object for tab completion. This means databunch.somefunc() ~ databunch.train_dl.somefunc()\n",
    "    _xtra = 'one_batch show_batch dataset'.split() # only tab completion these things.\n",
    "    # if no _xtra, then databunch instance can access ALL attributes and functions of _default (also all tab-completion)\n",
    "    # TODO: not sure if _xtra still works in current version of fastai2\n",
    "    \n",
    "    def __init__(self, *dls, path='.', device=None): self.dls,self.path = dls,Path(path)\n",
    "        #note: you can pass as many dataloader as you like\n",
    "        # dls will be stored as arrays and can be accessed by databunch instance itself, such as dbch[0],dbch[1] ...\n",
    "        # this helps with add_props below\n",
    "    def __getitem__(self, i): return self.dls[i]\n",
    "    def new_empty(self):\n",
    "        dls = [dl.new(dl.dataset.new_empty()) for dl in self.dls]\n",
    "        return type(self)(*dls)\n",
    "    \n",
    "\n",
    "    # add_props: add property (see above)\n",
    "    train_dl,valid_dl = add_props(lambda i,x: x[i])\n",
    "    # equivalent to this\n",
    "    # @property def train_dl(self): return self[0]\n",
    "    # @property def valid_dl(self): return self[1]\n",
    "    \n",
    "    train_ds,valid_ds = add_props(lambda i,x: x[i].dataset)\n",
    "    \n",
    "    def cuda(self, device=None):\n",
    "        for dl in self.dls: dl.device = default_device() if device is None else device\n",
    "        return self\n",
    "    \n",
    "    @classmethod\n",
    "    @delegates(TfmdDL.__init__)\n",
    "    def from_dblock(cls, dblock, source, path='.', type_tfms=None, item_tfms=None, batch_tfms=None, **kwargs):\n",
    "        return dblock.databunch(source, path=path, type_tfms=type_tfms, item_tfms=item_tfms, batch_tfms=batch_tfms, **kwargs)\n",
    "\n",
    "    _docs=dict(__getitem__=\"Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)\",\n",
    "               train_dl=\"Training `DataLoader`\",\n",
    "               valid_dl=\"Validation `DataLoader`\",\n",
    "               train_ds=\"Training `Dataset`\",\n",
    "               valid_ds=\"Validation `Dataset`\",\n",
    "               cuda=\"Use `device` (defaults to `default_device()`)\",\n",
    "               new_empty=\"Create a new empty version of `self` with the same transforms\",\n",
    "               from_dblock=\"Create a databunch from a given `dblock`\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New: dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@docs\n",
    "class DataLoaders(GetAttr):\n",
    "    # GetAttr: wrapper around __getattr__, using '_default' to set what object will be used in default\n",
    "    \"Basic wrapper around several `DataLoader`s.\"\n",
    "    _default='train' # default object for tab completion. This means dataloaders.somefunc() ~ dataloaders.train.somefunc()\n",
    "    def __init__(self, *loaders, path='.', device=None):\n",
    "        #note: you can pass as many dataloader as you like\n",
    "        # loaders will be stored as arrays and can be accessed by dataloaders instance itself, such as dls[0],dls[1] ...\n",
    "        # this helps with add_props below\n",
    "        self.loaders,self.path = loaders,Path(path)\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, i): return self.loaders[i]\n",
    "    def new_empty(self):\n",
    "        loaders = [dl.new(dl.dataset.new_empty()) for dl in self.loaders]\n",
    "        return type(self)(*loaders, path=self.path, device=self.device)\n",
    "\n",
    "    # add_props: add property (see above)\n",
    "    # equivalent to this\n",
    "    # @property def train(self): return self[0]\n",
    "    # @property def valid(self): return self[1]  \n",
    "    train   ,valid    = add_props(lambda i,x: x[i])\n",
    "    \n",
    "    \n",
    "    train_ds,valid_ds = add_props(lambda i,x: x[i].dataset)\n",
    "\n",
    "    @property\n",
    "    def device(self): return self._device\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, d):\n",
    "        for dl in self.loaders: dl.device = d\n",
    "        self._device = d\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        self.device = default_device() if device is None else device\n",
    "        return self\n",
    "\n",
    "    def cpu(self): return self.cuda(device=torch.device('cpu'))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dsets(cls, *ds, path='.',  bs=64, device=None, dl_type=TfmdDL, **kwargs):\n",
    "        default = (True,) + (False,) * (len(ds)-1)\n",
    "        defaults = {'shuffle': default, 'drop_last': default}\n",
    "        kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items()})\n",
    "        kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)]\n",
    "        return cls(*[dl_type(d, **k) for d,k in zip(ds, kwargs)], path=path, device=device)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dblock(cls, dblock, source, path='.',  bs=64, val_bs=None, shuffle_train=True, device=None, **kwargs):\n",
    "        return dblock.dataloaders(source, path=path, bs=bs, val_bs=val_bs, shuffle_train=shuffle_train, device=device, **kwargs)\n",
    "\n",
    "    _docs=dict(__getitem__=\"Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)\",\n",
    "               train=\"Training `DataLoader`\",\n",
    "               valid=\"Validation `DataLoader`\",\n",
    "               train_ds=\"Training `Dataset`\",\n",
    "               valid_ds=\"Validation `Dataset`\",\n",
    "               cuda=\"Use `device` (defaults to `default_device()`)\",\n",
    "               cpu=\"Use the cpu\",\n",
    "               new_empty=\"Create a new empty version of `self` with the same transforms\",\n",
    "               from_dblock=\"Create a dataloaders from a given `dblock`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=NegTfm(), bs=4, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(tdl,tdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dls.train.one_batch() # or dls[0].one_batch()\n",
    "x2 = next(iter(tdl))\n",
    "test_eq(x,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = dls.one_batch() # Note: quick tab completion, result of GetAttr. \n",
    "# This is basically dls.train.one_batch()\n",
    "test_eq(x,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1],\n",
       "         [-1],\n",
       "         [-1],\n",
       "         [-1]]),)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = dls.dataset # or this dataset will be 'default' as dbch.train_ds\n",
    "temp2 = dls.train_ds # == dls.train.dataset\n",
    "temp3 = dls.train.dataset\n",
    "test_eq(temp1,temp2)\n",
    "test_eq(temp1,temp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(TensorImage([1]),),\n",
       " (TensorImage([1]),),\n",
       " (TensorImage([1]),),\n",
       " (TensorImage([1]),)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train.dataset # note that NO tfms is done on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "on_batch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4d26a1a18a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# proper behavior on typo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai2/lib/python3.7/site-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xtra\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;31m#     def __getstate__(self): return self.__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: on_batch"
     ]
    }
   ],
   "source": [
    "dls.on_batch() # proper behavior on typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class A(Transform): \n",
    "    def encodes(self, x): return x \n",
    "    def decodes(self, x): return TitledInt(x) # TitledInt: int that can be shown as title\n",
    "\n",
    "@Transform\n",
    "def f(x)->None: return Tuple((x,x))\n",
    "\n",
    "start = torch.arange(50)\n",
    "test_eq_type(f(2), Tuple((2,2)))\n",
    "\n",
    "a = A()\n",
    "tdl = TfmdDL(start, after_item=lambda x: (a(x), f(x)), bs=4)\n",
    "x,y = tdl.one_batch()\n",
    "test_eq(type(y), Tuple)\n",
    "\n",
    "s = tdl.decode_batch((x,y))\n",
    "test_eq(type(s[0][1]), Tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfmdLists (train and val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A TfmdLists combines a collection of object with a transformation Pipeline.**\n",
    "- tfms applied only when index (lazy)\n",
    "- Try to have Pytorch Dataset behavior?\n",
    "- tfms can either be a Pipeline or a list of transforms, in which case, it will wrap them in a Pipeline. \n",
    "\n",
    "- use_list is passed along to L with the items, \n",
    "- as_item and split_idx are passed to each transform of the Pipeline. \n",
    "- do_setup indicates if the Pipeline.setup method should be called during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#export\n",
    "class FilteredBase:\n",
    "    \"Base class for lists with subsets\"\n",
    "    _dl_type,_dbunch_type = TfmdDL,DataLoaders\n",
    "    def __init__(self, *args, dl_type=None, **kwargs):\n",
    "        if dl_type is not None: self._dl_type = dl_type\n",
    "        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def n_subsets(self): return len(self.splits)\n",
    "    def _new(self, items, **kwargs): return super()._new(items, splits=self.splits, **kwargs)\n",
    "    def subset(self): raise NotImplemented\n",
    "\n",
    "    def dataloaders(self, bs=64, val_bs=None, shuffle_train=True, n=None, path='.', dl_type=None, dl_kwargs=None, device=None,\n",
    "                  **kwargs):\n",
    "        if device is None: device=default_device()\n",
    "        if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets\n",
    "        if dl_type is None: dl_type = self._dl_type\n",
    "        dl = dl_type(self.subset(0), bs=bs, shuffle=shuffle_train, drop_last=shuffle_train, n=n, device=device,\n",
    "                     **merge(kwargs, dl_kwargs[0]))\n",
    "        dls = [dl] + [dl.new(self.subset(i), bs=(bs if val_bs is None else val_bs), shuffle=False, drop_last=False, \n",
    "                             n=None, **dl_kwargs[i]) for i in range(1, self.n_subsets)]\n",
    "        return self._dbunch_type(*dls, path=path, device=device)\n",
    "\n",
    "FilteredBase.train,FilteredBase.valid = add_props(lambda i,x: x.subset(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TfmdLists(FilteredBase, L, GetAttr):\n",
    "    \"A `Pipeline` of `tfms` applied to a collection of `items`\"\n",
    "    _default='tfms'\n",
    "    def __init__(self, items, tfms, use_list=None, do_setup=True, as_item=True, split_idx=None, train_setup=True,\n",
    "                 splits=None, types=None, verbose=False):\n",
    "        super().__init__(items, use_list=use_list)\n",
    "        self.splits = L([slice(None),[]] if splits is None else splits).map(mask2idxs)\n",
    "        if isinstance(tfms,TfmdLists): tfms = tfms.tfms\n",
    "        if isinstance(tfms,Pipeline): do_setup=False\n",
    "        \n",
    "        self.tfms = Pipeline(tfms, as_item=as_item, split_idx=split_idx)\n",
    "        # why we pass split_idx into Pipeline? Go to class Pipeline in 6_04_transform.ipynb\n",
    "        \n",
    "        self.types = types\n",
    "        if do_setup: \n",
    "            pv(f\"Setting up {self.tfms}\", verbose)\n",
    "            self.setup(train_setup=train_setup)\n",
    "\n",
    "    def _new(self, items, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, types=self.types, **kwargs)\n",
    "    def subset(self, i): return self._new(self._get(self.splits[i]), split_idx=i)\n",
    "    def _after_item(self, o): return self.tfms(o)\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}: {self.items}\\ntfms - {self.tfms.fs}\"\n",
    "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
    "    def show(self, o, **kwargs): return self.tfms.show(o, **kwargs)\n",
    "    def decode(self, o, **kwargs): return self.tfms.decode(o, **kwargs)\n",
    "    def __call__(self, o, **kwargs): return self.tfms.__call__(o, **kwargs)\n",
    "    def overlapping_splits(self): return L(Counter(self.splits.concat()).values()).filter(gt(1))\n",
    "\n",
    "    def setup(self, train_setup=True):\n",
    "        self.tfms.setup(self, train_setup)\n",
    "        if len(self) != 0:\n",
    "            x = super().__getitem__(0) if self.splits is None else super().__getitem__(self.splits[0])[0]\n",
    "            self.types = []\n",
    "            for f in self.tfms.fs:\n",
    "                self.types.append(getattr(f, 'input_types', type(x)))\n",
    "                x = f(x)\n",
    "            self.types.append(type(x))\n",
    "        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n",
    "        self.pretty_types = '\\n'.join([f'  - {t}' for t in types])\n",
    "\n",
    "    def infer_idx(self, x):\n",
    "        idx = 0\n",
    "        for t in self.types:\n",
    "            if isinstance(x, t): break\n",
    "            idx += 1\n",
    "        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n",
    "        pretty_types = '\\n'.join([f'  - {t}' for t in types])\n",
    "        assert idx < len(self.types), f\"Expected an input of type in \\n{pretty_types}\\n but got {type(x)}\"\n",
    "        return idx\n",
    "\n",
    "    def infer(self, x):\n",
    "        return compose_tfms(x, tfms=self.tfms.fs[self.infer_idx(x):], split_idx=self.split_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        res = super().__getitem__(idx)\n",
    "        if self._after_item is None: return res\n",
    "        return self._after_item(res) if is_indexer(idx) else res.map(self._after_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs(TfmdLists,\n",
    "         setup=\"Transform setup with self\",\n",
    "         decode=\"From `Pipeline\",\n",
    "         show=\"From `Pipeline\",\n",
    "         overlapping_splits=\"All splits that are in more than one split\",\n",
    "         subset=\"New `TfmdLists` with same tfms that only includes items in `i`th split\",\n",
    "         infer_idx=\"Finds the index where `self.tfms` can be applied to `x`, depending on the type of `x`\",\n",
    "         infer=\"Apply `self.tfms` to `x` starting at the right tfm depending on the type of `x`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#exports\n",
    "def decode_at(o, idx):\n",
    "    \"Decoded item at `idx`\"\n",
    "    return o.decode(o[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def show_at(o, idx, **kwargs):\n",
    "    \"Show item at `idx`\",\n",
    "    return o.show(o[idx], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _IntFloatTfm(Transform):\n",
    "    def encodes(self, o):  return TitledInt(o)\n",
    "    def decodes(self, o):  return TitledFloat(o)\n",
    "int2f_tfm=_IntFloatTfm()\n",
    "\n",
    "def _neg(o): return -o\n",
    "neg_tfm = Transform(_neg, _neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use TfmdList\n",
    "\n",
    "items = L([1.,2.,3.])\n",
    "tfms = [neg_tfm, int2f_tfm]\n",
    "tl = TfmdLists(items, tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfmdLists: [1.0, 2.0, 3.0]\n",
       "tfms - (#2) [Transform: True (object,object) -> _neg (object,object) -> _neg,_IntFloatTfm: True (object,object) -> encodes (object,object) -> decodes]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, -2, -3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl[0],tl[1],tl[2] # transforms applied only when index (lazy)\n",
    "\n",
    "# TODO: test if TfmdList can reserve type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fastai2.torch_core.TitledInt,\n",
       " fastai2.torch_core.TitledInt,\n",
       " fastai2.torch_core.TitledInt)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tl[0]),type(tl[1]),type(tl[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, fastai2.torch_core.TitledFloat)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.decode(tl[0]),type(tl.decode(tl[0])) # change to TitledFloat then negated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[float, float, fastai2.torch_core.TitledInt]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.types # TODO the fuck is this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: (#2) [Transform: True (object,object) -> _neg (object,object) -> _neg,_IntFloatTfm: True (object,object) -> encodes (object,object) -> decodes]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.tfms # auto turn tfms into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(tl[0], TitledInt(-1))\n",
    "test_eq_type(tl[1], TitledInt(-2))\n",
    "test_eq_type(tl.decode(tl[2]), TitledFloat(3.))\n",
    "test_stdout(lambda: show_at(tl, 2), '-3')\n",
    "test_eq(tl.types, [float, float, TitledInt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add train/val splits to TfmdLists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#3) [1.0,2.0,3.0],\n",
       " [Transform: True (object,object) -> _neg (object,object) -> _neg,\n",
       "  _IntFloatTfm: True (object,object) -> encodes (object,object) -> decodes])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items,tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = [[0,2],[1]]\n",
    "tl = TfmdLists(items, tfms=tfms, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TfmdLists: [1.0, 3.0]\n",
       " tfms - (#2) [Transform: True (object,object) -> _neg (object,object) -> _neg,_IntFloatTfm: True (object,object) -> encodes (object,object) -> decodes],\n",
       " [1.0, 3.0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.train,tl.train.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline: (#2) [Transform: True (object,object) -> _neg (object,object) -> _neg,_IntFloatTfm: True (object,object) -> encodes (object,object) -> decodes],\n",
       " Pipeline: (#2) [Transform: True (object,object) -> _neg (object,object) -> _neg,_IntFloatTfm: True (object,object) -> encodes (object,object) -> decodes])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.train.tfms,tl.valid.tfms # train and val have A COPY OF a same pipeline (see cell below)\n",
    "# implying we can have different pipelines for train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0x7f0bfa17f8d0', '0x7f0bfa17fa50')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(id(tl.train.tfms)),hex(id(tl.valid.tfms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tl.n_subsets, 2)\n",
    "test_eq(tl.train, tl.subset(0)) #with splits, tfmdlist can auto set train and valid\n",
    "test_eq(tl.valid, tl.subset(1))\n",
    "\n",
    "test_eq(tl.train.items, items[splits[0]])\n",
    "test_eq(tl.valid.items, items[splits[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastcore.transform.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tl.train.tfms))\n",
    "test_eq(tl.train.tfms.split_idx, 0) # train tfms pipeline knows that it is executed on train dataset\n",
    "# meaning EACH tfm in train pipeline knows this\n",
    "test_eq(tl.valid.tfms.split_idx, 1) # val tfms pipeline knows ...\n",
    "\n",
    "# all thanks to class Pipeline and compose functions in 6_04_transform notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(tl.splits, L(splits))\n",
    "assert not tl.overlapping_splits() # check split overlapping. Cool feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  2\n",
       "1  2  3\n",
       "2  3  4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dict(a=[1,2,3],b=[2,3,4]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TfmdLists(df, tfms = lambda o: o.a+1, splits=[[0],[1,2]]) # take dataframe input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfmdLists:    a  b\n",
       "0  1  2\n",
       "1  2  3\n",
       "2  3  4\n",
       "tfms - (#1) [Transform: True (object,object) -> <lambda> ]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl # simple tfm: increase by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  2\n",
       "1  2  3\n",
       "2  3  4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl[0],tl[1],tl[2] # column a only (post tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [2,3,4]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl[:] # get everything post tfm as L list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tl[1,2], [3,4])\n",
    "tr = tl.subset(0)\n",
    "test_eq(tr[:], [2])\n",
    "val = tl.subset(1)\n",
    "test_eq(val[:], [3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [1.0,2.0,3.0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _B(Transform):\n",
    "    def __init__(self): self.m = 0\n",
    "    def encodes(self, o): return o+self.m\n",
    "    def decodes(self, o): return o-self.m\n",
    "    def setups(self, items): self.m = tensor(items).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for setup, which updates `self.m`\n",
    "tl = TfmdLists(items, _B())\n",
    "test_eq(tl.m, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tl.m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's how we can use `TfmdList.setup` to implement a simple category list, getting labels from a mock file list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Cat(Transform):\n",
    "    order = 1\n",
    "    def encodes(self, o):    return int(self.o2i[o])\n",
    "    def decodes(self, o):    return TitledStr(self.vocab[o])\n",
    "    def setups(self, items): self.vocab,self.o2i = uniqueify(L(items), sort=True, bidir=True)\n",
    "tcat = _Cat()\n",
    "\n",
    "def _lbl(o): return TitledStr(o.split('_')[0]) # no order given ==> order = 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that tfms are sorted by `order` & `_lbl` is called first\n",
    "fns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','dog_1.jpg']\n",
    "tl = TfmdLists(fns, [tcat,_lbl])\n",
    "exp_voc = ['cat','dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0x7f0bf2bc9550', '0x7f0bf2bc9550', '0x7f0bf2bc9550')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(id(tl.vocab)), hex(id(tl.tfms.vocab)), hex(id(tcat.vocab))\n",
    "# tfm class var can be accessed by: tfmdList obj, tfmdList's pipeline, and of course the tfm itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#2) ['cat','dog'], {'cat': 0, 'dog': 1})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.vocab, tl.o2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: (#2) [Transform: True (object,object) -> _lbl ,_Cat: True (object,object) -> encodes (object,object) -> decodes]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test setup to see whether vocab is created\n",
    "test_eq(tcat.vocab, exp_voc)\n",
    "test_eq(tl.tfms.vocab, exp_voc)\n",
    "test_eq(tl.vocab, exp_voc)\n",
    "\n",
    "\n",
    "test_eq(tl, (1,0,0,0,1))\n",
    "test_eq([tl.decode(o) for o in tl], ('dog','cat','cat','cat','dog'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT NOTE: Check only the training set is taken into account for Pipeline SETUPS (best practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TfmdLists(fns, [tcat,_lbl], splits=[[0,4], [1,2,3]])\n",
    "test_eq(tcat.vocab, ['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.arange(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(Transform): \n",
    "    def encodes(self, x): return x+1\n",
    "    def decodes(self, x): return TitledInt(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use both tfmdLists and TfmdDL + The consequences of matching split_idx for different tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = NegTfm(split_idx=0) # set split_idx for NegTfm (if there is train/val set, this tfm is only used on train set aka subset 0)\n",
    "tds = TfmdLists(start, A())\n",
    "tdl = TfmdDL(tds, after_batch=tfm, bs=4) #after_batch: run AFTER tuples being collated together in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tds.split_idx)\n",
    "print(tdl.split_idx)\n",
    "print(tfm.split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tdl.one_batch()\n",
    "x # increased by one, but not neg because those tfms' split idx dont match (see cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -2, -3, -4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds.split_idx = 0 # set tfmdlist's tfm (A()) split_idx to match NegTfm split_idx => both tfms will be executed\n",
    "# note that tfmdDL split_idx is auto set to be same as tfmdLists split_idx\n",
    "x = tdl.one_batch()\n",
    "x #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tds.split_idx)\n",
    "print(tdl.split_idx)\n",
    "print(tfm.split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tds.split_idx = 1\n",
    "print(tds.split_idx)\n",
    "print(tdl.split_idx) \n",
    "print(tfm.split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tdl.one_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tfm = NegTfm() # DON'T set split_idx for tfm\n",
    "tds = TfmdLists(start, A())\n",
    "tdl = TfmdDL(tds, after_batch=tfm, bs=4)\n",
    "print(tds.split_idx)\n",
    "print(tdl.split_idx)\n",
    "print(tfm.split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -2, -3, -4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tdl.one_batch() # due to matching split_idx in cell above, all tfms are performed\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets (creating X and y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally for a dataset when you index into it, you should have independent var and dependent var returned to you\n",
    "\n",
    "This is where that happens, **using multiple tfmdList with multiple pipelines (ideally 2, 1 for indepedent and 1 for dependent)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@docs\n",
    "@delegates(TfmdLists)\n",
    "class Datasets(FilteredBase):\n",
    "    \"A dataset that creates a tuple from each `tfms`, passed thru `item_tfms`\"\n",
    "    def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs):\n",
    "        super().__init__(dl_type=dl_type)\n",
    "        self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "        # few tfmdLists are created. Technically 2 are created, each tfmlist for each \"list of transforms\" (aka pipeline) you passed in\n",
    "        # 1 tfmlist for X tfms pipeline, 1 for y tfms pipeline\n",
    "        # Example:\n",
    "#         tfms = [[PILImage.create], [labeller, Categorize()]]\n",
    "#         pets = Datasets(items, tfms)\n",
    "        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "\n",
    "    def __getitem__(self, it):\n",
    "        res = tuple([tl[it] for tl in self.tls])\n",
    "        return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n",
    "    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n",
    "    def __len__(self): return len(self.tls[0])\n",
    "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
    "    def __repr__(self): return coll_repr(self)\n",
    "    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n",
    "    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp)\n",
    "    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n",
    "    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n",
    "    @property\n",
    "    def splits(self): return self.tls[0].splits\n",
    "    @property\n",
    "    def split_idx(self): return self.tls[0].tfms.split_idx\n",
    "    @property\n",
    "    def items(self): return self.tls[0].items\n",
    "    @items.setter\n",
    "    def items(self, v):\n",
    "        for tl in self.tls: tl.items = v\n",
    "\n",
    "    def show(self, o, ctx=None, **kwargs):\n",
    "        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n",
    "        return ctx\n",
    "\n",
    "    def new_empty(self):\n",
    "        tls = [tl._new([], split_idx=tl.split_idx) for tl in self.tls]\n",
    "        return type(self)(tls=tls, n_inp=self.n_inp)\n",
    "\n",
    "    @contextmanager\n",
    "    def set_split_idx(self, i):\n",
    "        old_split_idx = self.split_idx\n",
    "        for tl in self.tls: tl.tfms.split_idx = i\n",
    "        yield self\n",
    "        for tl in self.tls: tl.tfms.split_idx = old_split_idx\n",
    "\n",
    "    _docs=dict(\n",
    "        decode=\"Compose `decode` of all `tuple_tfms` then all `tfms` on `i`\",\n",
    "        show=\"Show item `o` in `ctx`\",\n",
    "        dataloaders=\"Get a `DataLoaders`\",\n",
    "        overlapping_splits=\"All splits that are in more than one split\",\n",
    "        subset=\"New `Datasets` that only includes subset `i`\",\n",
    "        new_empty=\"Create a new empty version of the `self`, keeping only the transforms\",\n",
    "        set_split_idx=\"Contextmanager to use the same `Datasets` with another `split_idx`\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Datasets` creates **a tuple from `items` (typically input,target)** by applying to them each list of `Transform` (or `Pipeline`) in `tfms`. \n",
    "\n",
    "Note that **if `tfms` contains only one list of `tfms`, the items given by `Datasets` will be tuples of one element.**\n",
    "\n",
    "`n_inp` is the number of elements in the tuples that should be considered part of the input and will default to 1 if `tfms` consists of one set of transforms, `len(tfms)-1` otherwise. In most cases, the number of elements in the tuples spit out by `Datasets` will be 2 (for input,target) but it can happen that there is 3 (Siamese networks or tabular data) in which case we need to be able to determine when the inputs end and the targets begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _IntFloatTfm(Transform):\n",
    "    def encodes(self, o):  return TitledInt(o)\n",
    "    def decodes(self, o):  return TitledFloat(o)\n",
    "int2f_tfm=_IntFloatTfm()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [1,2,3,4]\n",
    "dsets = Datasets(items, [[neg_tfm,int2f_tfm], [add(1)]]) # 2 set of tfms aka 2 pipelines\n",
    "t = dsets[0] # 2 pipelines applied only when index (lazy, because tfmdlist is used)\n",
    "test_eq(t, (-1,2)) # 2 results b/c 2 pipelines on the same item (items[0])\n",
    "\n",
    "test_eq(dsets[0,1,2], [(-1,2),(-2,3),(-3,4)])\n",
    "test_eq(dsets.n_inp, 1)\n",
    "dsets.decode(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfmdLists: [1, 2, 3, 4]\n",
       "tfms - (#2) [Transform: True (object,object) -> _neg (object,object) -> _neg,_IntFloatTfm: True (object,object) -> encodes (object,object) -> decodes]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.tls[0] # 2 tfmdlist b/c of 2 pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfmdLists: [1, 2, 3, 4]\n",
       "tfms - (#1) [Transform: True (object,object) -> <lambda> ]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.tls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline: (#2) [Transform: True (object,object) -> _neg (object,object) -> _neg,_IntFloatTfm: True (object,object) -> encodes (object,object) -> decodes],\n",
       " Pipeline: (#1) [Transform: True (object,object) -> <lambda> ])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.tls[0].tfms,dsets.tls[1].tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#4) [(-1, 2),(-2, 3),(-3, 4),(-4, 5)], (#0) [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.train,dsets.valid # no split so nothing in val. Note that tfms already applied when you do .train or .valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _IntFloatTfm(Transform):\n",
    "    def encodes(self, o):  return TitledInt(o)\n",
    "    def decodes(self, o):  return TitledFloat(o)\n",
    "int2f_tfm=_IntFloatTfm()\n",
    "\n",
    "def _neg(o): return -o\n",
    "neg_tfm = Transform(_neg, _neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(Transform):\n",
    "    def encodes(self, o): return (o-self.m)/self.s\n",
    "    def decodes(self, o): return (o*self.s)+self.m\n",
    "    def setups(self, items):\n",
    "        its = tensor(items).float()\n",
    "        self.m,self.s = its.mean(),its.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [1,2,3,4]\n",
    "nrm = Norm()\n",
    "dsets = Datasets(items, [[neg_tfm,int2f_tfm], [neg_tfm,nrm]])\n",
    "x,y = zip(*dsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-1, -2, -3, -4),\n",
       " (tensor(1.1619), tensor(0.3873), tensor(-0.3873), tensor(-1.1619)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai2.torch_core.TitledInt"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x[0]) # correct type b/c of int2f_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.5000), tensor(1.2910))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrm.m,nrm.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2, tensor(0.3873))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets[1] # can be also indexed into, normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = zip(*dsets)\n",
    "test_close(tensor(y).mean(), 0)\n",
    "test_close(tensor(y).std(), 1)\n",
    "test_eq(x, (-1,-2,-3,-4,))\n",
    "test_eq(nrm.m, -2.5)\n",
    "test_stdout(lambda:show_at(dsets, 1), '-2')\n",
    "\n",
    "test_eq(dsets.m, nrm.m)\n",
    "test_eq(dsets.norm.m, nrm.m)\n",
    "test_eq(dsets.train.norm.m, nrm.m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfms' split_idx when using Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Check filtering is properly applied\n",
    "class B(Transform):\n",
    "    def encodes(self, x)->None:  return int(x+1)\n",
    "    def decodes(self, x):        return TitledInt(x-1)\n",
    "    \n",
    "add1 = B(split_idx=1) # different split_idx ( = 1 for val set only)\n",
    "\n",
    "dsets = Datasets(items, [neg_tfm, [neg_tfm,int2f_tfm,add1]], splits=[[3],[0,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [(-1, -1),(-2, -2),(-3, -3),(-4, -4)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets # add1 tfm is not applied when calling the entire dsets due to different split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [(-4, -4)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.train # add1 tfm not executed on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [(-1, 0),(-2, -1),(-3, -2)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.valid # add1 tmf run on valid when use dsets.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fns = ['dog_0.jpg','cat_0.jpg','cat_2.jpg','cat_1.jpg','kid_1.jpg']\n",
    "tcat = _Cat()\n",
    "dsets = Datasets(test_fns, [[tcat,_lbl]], splits=[[0,1,2], [3,4]])\n",
    "test_eq(tcat.vocab, ['cat','dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#3) [(1,),(0,),(0,)]\n",
      "[(0,)]\n"
     ]
    }
   ],
   "source": [
    "print(dsets.train) # return tuple of 1 for each because there is only 1 pipeline [tcat_,_lbl]\n",
    "print(dsets.valid[:-1]) \n",
    "# dict KeyError when trying to show the last value of dsets or the last value of dsets.valid \n",
    "# because vocab is built on train set which only has cat and dog. Valid set has new value 'kid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stdout(lambda: show_at(dsets.train, 0), \"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [0,1,2,3,4]\n",
    "dsets = Datasets(inp, tfms=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(*dsets[2], 2)          # Retrieve one item (subset 0 is the default)\n",
    "test_eq(dsets[1,2], [(1,),(2,)])    # Retrieve two items by index\n",
    "mask = [True,False,False,True,False]\n",
    "test_eq(dsets[mask], [(0,),(3,)])   # Retrieve two items by mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Temp.temp_func of <__main__.Temp object at 0x7f0bf212cc50>>\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# understanding attrgetter: retrieve attribute with given name from an obj\n",
    "class Temp:\n",
    "    b=2\n",
    "    def __init__(self):\n",
    "        self.a = 1\n",
    "    def temp_func(self):\n",
    "        print(self.a)\n",
    "print(attrgetter('temp_func')(Temp()))\n",
    "print(attrgetter('a')(Temp()))\n",
    "print(attrgetter('b')(Temp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a\n",
       "0  5\n",
       "1  1\n",
       "2  2\n",
       "3  3\n",
       "4  4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = pd.DataFrame(dict(a=[5,1,2,3,4]))\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = Datasets(inp, tfms=attrgetter('a')) # tfm here mean: get column a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) [(5,),(1,),(2,),(3,),(4,)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(dsets[1,2], [(1,),(2,)])    # Retrieve two items by index\n",
    "mask = [True,False,False,True,False]\n",
    "test_eq(dsets[mask], [(5,),(3,)])   # Retrieve two items by mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_inp\n",
    "\n",
    "`n_inp` is the number of elements in the tuples that should be considered part of the input and will default to 1 if `tfms` consists of one set of transforms, `len(tfms)-1` otherwise. In most cases, the number of elements in the tuples spit out by `Datasets` will be 2 (for input,target) but it can happen that there is 3 (Siamese networks or tabular data) in which case we need to be able to determine when the inputs end and the targets begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test n_inp\n",
    "inp = [0,1,2,3,4]\n",
    "dsets = Datasets(inp, tfms=[None])\n",
    "test_eq(dsets.n_inp, 1) # b/c only one set of tfms\n",
    "dsets = Datasets(inp, tfms=[[None],[None],[None]])\n",
    "test_eq(dsets.n_inp, 2) # 3 pipelines: first 2 for 2 inputs, last one for output (default)\n",
    "dsets = Datasets(inp, tfms=[[None],[None],[None]], n_inp=1)\n",
    "test_eq(dsets.n_inp, 1) # 3 pipelines, first 1 for input, last 2 for outputs (Siamese data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train/val splits for Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) [(0,),(1,),(2,),(3,),(4,)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splits can be indices\n",
    "dsets = Datasets(range(5), tfms=[None], splits=[tensor([0,2]), [1,3,4]])\n",
    "\n",
    "test_eq(dsets.subset(0), [(0,),(2,)])\n",
    "test_eq(dsets.train, [(0,),(2,)])       # Subset 0 is aliased to `train`\n",
    "test_eq(dsets.subset(1), [(1,),(3,),(4,)])\n",
    "test_eq(dsets.valid, [(1,),(3,),(4,)])     # Subset 1 is aliased to `valid`\n",
    "test_eq(*dsets.valid[2], 4)\n",
    "#assert '[(1,),(3,),(4,)]' in str(dsets) and '[(0,),(2,)]' in str(dsets)\n",
    "dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits can be boolean masks (they don't have to cover all items, BUT MUST BE DISJOINT)\n",
    "splits = [[False,True,True,False,True], [True,False,False,False,False]]\n",
    "dsets = Datasets(range(5), tfms=[None], splits=splits)\n",
    "\n",
    "test_eq(dsets.train, [(1,),(2,),(4,)])\n",
    "test_eq(dsets.valid, [(0,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transforms to all items\n",
    "tfm = [[lambda x: x*2,lambda x: x+1]] # double then add 1\n",
    "splits = [[1,2],[0,3,4]]\n",
    "dsets = Datasets(range(5), tfm, splits=splits)\n",
    "test_eq(dsets.train,[(3,),(5,)])\n",
    "test_eq(dsets.valid,[(1,),(7,),(9,)])\n",
    "test_eq(dsets.train[False,True], [(5,)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split_idx continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only transform val set aka subset 1 by setting split_idx = 1\n",
    "class _Tfm(Transform):\n",
    "    split_idx=1\n",
    "    def encodes(self, x): return x*2\n",
    "    def decodes(self, x): return TitledStr(x//2)\n",
    "\n",
    "# or you can set split_idx this way: tfm = _Tfm(split_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) [(0,),(1,),(2,),(3,),(4,)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets = Datasets(range(5), [_Tfm()], splits=[[1,2],[0,3,4]])\n",
    "test_eq(dsets.train,[(1,),(2,)]) # nothing happens\n",
    "test_eq(dsets.valid,[(0,),(6,),(8,)]) # tfm is applied\n",
    "test_eq(dsets.train[False,True], [(2,)])\n",
    "dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A context manager to change the split_idx and apply the validation transform on the training set **temporarily**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(dsets.split_idx)\n",
    "print(dsets.train.split_idx)\n",
    "print(dsets.valid.split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [(1,),(2,)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = dsets.train\n",
    "ds # nothing happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "with ds.set_split_idx(1): # switch split_idx of train from 0 to 1 temporarily. \n",
    "    # TODO this doesn't work: dsets.train.set_split_idx(1)\n",
    "    print(dsets.split_idx)\n",
    "    print(ds.split_idx) # this was switched from 0 to 1\n",
    "    print(dsets.valid.split_idx)\n",
    "    test_eq(ds,[(2,),(4,)])\n",
    "    \n",
    "test_eq(dsets.train,[(1,),(2,)]) #back to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Test Datasets pickles\n",
    "dsrc1 = pickle.loads(pickle.dumps(dsets))\n",
    "test_eq(dsets.train, dsrc1.train)\n",
    "test_eq(dsets.valid, dsrc1.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only transform subset 1 (val) by setting split_idx = 1\n",
    "class _Tfm(Transform):\n",
    "    split_idx=1\n",
    "    def encodes(self, x): return x*2\n",
    "    def decodes(self, x): return TitledStr(x//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#2) [(1, 1),(2, 2)]\n",
      "(#3) [(0, 0),(6, 3),(8, 4)]\n"
     ]
    }
   ],
   "source": [
    "dsets = Datasets(range(5), [_Tfm(),noop], splits=[[1,2],[0,3,4]]) # THis is actually 2 pipelines, equivalent to [[_Tfm()],[noop]]\n",
    "print(dsets.train)\n",
    "print(dsets.valid) # note that for val, only first pipeline _Tfm() is called to produce X val, \\\n",
    "# so 0 3 4 is double. Noop does nothing as all in y val production, so 0 3 4 is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1, -2, -3, -4]),)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = torch.arange(0,50)\n",
    "tds = Datasets(start, [A()]) # A() transformation is just adding 1\n",
    "tdl = TfmdDL(tds, after_item=NegTfm(), bs=4) # transform each item in tuple (where as_item=False)\n",
    "b = tdl.one_batch()\n",
    "b # note that since tds Datasets only have 1 tfm => only X is produced, so it returns tuple of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [(1,),(2,),(3,),(4,)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdl.decode_batch(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfms split_idx on DataLoaders (datasets.dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#8) [(0,),(1,),(2,),(3,),(4,),(5,),(6,),(7,)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only transform subset 1\n",
    "class _Tfm(Transform):\n",
    "    split_idx=1\n",
    "    def encodes(self, x): return x*2\n",
    "\n",
    "dsets = Datasets(range(8), [None], splits=[[1,2,5,7],[0,3,4,6]])\n",
    "dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dsets.dataloaders(bs=4, after_batch=_Tfm(), # tfm done after a batch is formed, normally done on GPU\n",
    "                        shuffle_train=False, device=torch.device('cpu'))\n",
    "\n",
    "test_eq(dls.train, [(tensor([1,2,5, 7]),)]) # 1 batch of train_dl: don't change\n",
    "test_eq(dls.valid, [(tensor([0,6,8,12]),)]) # 1 batch of val_dl: _Tfm() is used\n",
    "test_eq(dls.n_inp, 1)\n",
    "# TODO: check if dls.train is a TfmdDL since isn't it dls.train.one_batch()???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add test set for inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
