{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.torch_basics import *\n",
    "from fastai2.data.core import *\n",
    "from fastai2.data.load import *\n",
    "from fastai2.data.external import *\n",
    "from fastai2.data.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data block\n",
    "\n",
    "> High level API to quickly get your data in a `DataBunch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformBlock():\n",
    "    \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "    def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dbunch_kwargs=None):\n",
    "        self.type_tfms  =            L(type_tfms)\n",
    "        self.item_tfms  = ToTensor + L(item_tfms)\n",
    "        self.batch_tfms =            L(batch_tfms)\n",
    "        self.dl_type,self.dbunch_kwargs = dl_type,({} if dbunch_kwargs is None else dbunch_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CategoryBlock(vocab=None, add_na=False):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    return TransformBlock(type_tfms=Categorize(vocab=vocab, add_na=add_na))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MultiCategoryBlock(encoded=False, vocab=None, add_na=False):\n",
    "    \"`TransformBlock` for multi-label categorical targets\"\n",
    "    tfm = EncodedMultiCategorize(vocab=vocab) if encoded else [MultiCategorize(vocab=vocab, add_na=add_na), OneHotEncode]\n",
    "    return TransformBlock(type_tfms=tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from inspect import isfunction,ismethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _merge_tfms(*tfms):\n",
    "    \"Group the `tfms` in a single list, removing duplicates (from the same class) and instantiating\"\n",
    "    g = groupby(concat(*tfms), lambda o:\n",
    "        o if isinstance(o, type) else o.__qualname__ if (isfunction(o) or ismethod(o)) else o.__class__)\n",
    "    return L(v[-1] for k,v in g.items()).map(instantiate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For example, so not exported\n",
    "from fastai2.vision.core import *\n",
    "from fastai2.vision.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hide\n",
    "tfms = _merge_tfms([Categorize, MultiCategorize, Categorize(['dog', 'cat'])], Categorize(['a', 'b']))\n",
    "#If there are several instantiated versions, the last one is kept.\n",
    "test_eq(len(tfms), 2)\n",
    "test_eq(tfms[1].__class__, MultiCategorize)\n",
    "test_eq(tfms[0].__class__, Categorize)\n",
    "test_eq(tfms[0].vocab, ['a', 'b'])\n",
    "\n",
    "tfms = _merge_tfms([PILImage.create, PILImage.show])\n",
    "#Check methods are properly separated\n",
    "test_eq(len(tfms), 2)\n",
    "tfms = _merge_tfms([show_image, set_trace])\n",
    "#Check functions are properly separated\n",
    "test_eq(len(tfms), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@docs\n",
    "@funcs_kwargs\n",
    "class DataBlock():\n",
    "    \"Generic container to quickly build `DataSource` and `DataBunch`\"\n",
    "    get_x=get_items=splitter=get_y = None\n",
    "    dl_type = TfmdDL\n",
    "    _methods = 'get_items splitter get_y get_x'.split()\n",
    "    def __init__(self, blocks=None, dl_type=None, getters=None, n_inp=None, **kwargs):\n",
    "        blocks = L(getattr(self,'blocks',(TransformBlock,TransformBlock)) if blocks is None else blocks)\n",
    "        blocks = L(b() if callable(b) else b for b in blocks)\n",
    "        self.default_type_tfms = blocks.attrgot('type_tfms', L())\n",
    "        self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n",
    "        self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n",
    "        for t in blocks:\n",
    "            if getattr(t, 'dl_type', None) is not None: self.dl_type = t.dl_type\n",
    "        if dl_type is not None: self.dl_type = dl_type\n",
    "        self.databunch = delegates(self.dl_type.__init__)(self.databunch)\n",
    "        self.dbunch_kwargs = merge(*blocks.attrgot('dbunch_kwargs', {}))\n",
    "        self.n_inp,self.getters = n_inp,L(getters)\n",
    "        if getters is not None: assert self.get_x is None and self.get_y is None\n",
    "        assert not kwargs\n",
    "\n",
    "    def datasource(self, source, type_tfms=None):\n",
    "        self.source = source\n",
    "        items = (self.get_items or noop)(source)\n",
    "        if isinstance(items,tuple):\n",
    "            items = L(items).zip()\n",
    "            labellers = [itemgetter(i) for i in range_of(self.default_type_tfms)]\n",
    "        else: labellers = [noop] * len(self.default_type_tfms)\n",
    "        splits = (self.splitter or noop)(items)\n",
    "        if self.get_x:   labellers[0] = self.get_x\n",
    "        if self.get_y:   labellers[1] = self.get_y\n",
    "        if self.getters: labellers = self.getters\n",
    "        if type_tfms is None: type_tfms = [L() for t in self.default_type_tfms]\n",
    "        type_tfms = L([self.default_type_tfms, type_tfms, labellers]).map_zip(\n",
    "            lambda tt,tfm,l: L(l) + _merge_tfms(tt, tfm))\n",
    "        return DataSource(items, tfms=type_tfms, splits=splits, dl_type=self.dl_type, n_inp=self.n_inp)\n",
    "\n",
    "    def databunch(self, source, path='.', type_tfms=None, item_tfms=None, batch_tfms=None, **kwargs):\n",
    "        dsrc = self.datasource(source, type_tfms=type_tfms)\n",
    "        item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n",
    "        batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n",
    "        kwargs = {**self.dbunch_kwargs, **kwargs}\n",
    "        return dsrc.databunch(path=path, after_item=item_tfms, after_batch=batch_tfms, **kwargs)\n",
    "\n",
    "    _docs = dict(datasource=\"Create a `Datasource` from `source` with `type_tfms`\",\n",
    "                 databunch=\"Create a `DataBunch` from `source` with `item_tfms` and `batch_tfms`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The use of DataBlock: to automatically construct a DataSource of DataBunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a `DataBlock` you need to give the library four things: the types of your input/labels then at least two functions: `get_items` and `splitter`. You may also need to include `get_x` and `get_y` or a more generic list of `getters` that are applied to the results of `get_items`.\n",
    "\n",
    "Once those are provided, you automatically get a `DataSource` or a `DataBunch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "dblock = DataBlock()\n",
    "show_doc(dblock.databunch, name=\"DataBlock.databunch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `DataBlock` by passing functions or subclassing. The two following data blocks are the same for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(DataBlock):\n",
    "    blocks = ImageBlock(cls=PILImageBW),CategoryBlock #TODO: the fuck is this\n",
    "    \n",
    "    def get_items(self, source): \n",
    "        # get all items (for MNIST it's a list of paths)\n",
    "        # source is the path where items are located\n",
    "        return get_image_files(Path(source))\n",
    "    \n",
    "    def splitter (self, items ): \n",
    "        # use a train/val splitter function on the list of items.\n",
    "        # return list of 2: 1 for train, 1 for val\n",
    "        return GrandparentSplitter()(items)\n",
    "    \n",
    "    def get_y    (self, item  ): \n",
    "        # input: 1 single item\n",
    "        # output: the label of that one item \n",
    "        # if multi-label, get the list of labels, e.g.: if item is [X,y], get_y = lambda x:x[1].split(' '))\n",
    "        return parent_label(item)\n",
    "    \n",
    "#     def get_x    (self, item  ): pass\n",
    "    # you can provide a get_x function: given 1 single item, return the X portion of that item\n",
    "    # e.g. if item is a tuple (X,y), return item[0] aka X\n",
    "mnist = MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent to\n",
    "mnist = DataBlock(blocks = (ImageBlock(cls=PILImageBW),CategoryBlock),\n",
    "                  get_items = get_image_files,\n",
    "                  splitter = GrandparentSplitter(),\n",
    "                  get_y = parent_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each type comes with default transforms that will be applied\n",
    "- at the base level to create items in a tuple (usually input,target) from the base elements (like filenames)\n",
    "- at the item level of the datasource\n",
    "- at the batch level\n",
    "\n",
    "They are called respectively type transforms, item transforms, batch transforms. In the case of MNIST, the type transforms are the method to create a `PILImageBW` (for the input) and the `Categorize` transform (for the target), the item transform is `ToTensor` and the batch transforms are `Cuda` and `IntToFloatTensor`. You can add any other transforms by passing them in `DataBlock.datasource` or `DataBlock.databunch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mnist.default_type_tfms[0], [PILImageBW.create])\n",
    "test_eq(mnist.default_type_tfms[1].map(type), [Categorize])\n",
    "test_eq(mnist.default_item_tfms.map(type), [ToTensor])\n",
    "test_eq(mnist.default_batch_tfms.map(type), [IntToFloatTensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create datasource or databunch using datablock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = MNIST().datasource(untar_data(URLs.MNIST_TINY)) #TODO lol bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = MNIST().databunch(untar_data(URLs.MNIST_TINY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsrc = MNIST().datasource(untar_data(URLs.MNIST_TINY))\n",
    "# test_eq(dsrc.vocab, ['3', '7'])\n",
    "# x,y = dsrc.train[0]\n",
    "# test_eq(x.size,(28,28))\n",
    "# show_at(dsrc.train, 0, cmap='Greys', figsize=(2,2));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
