{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.torch_basics import *\n",
    "from fastai2.data.core import *\n",
    "from fastai2.data.load import *\n",
    "from fastai2.data.external import *\n",
    "from fastai2.data.transforms import *\n",
    "\n",
    "\n",
    "# TODO: fix add_props in fastcore utils so class Dataloaders can work\n",
    "# def add_props(f, g=None, n=2):\n",
    "#     \"Create properties passing each of `range(n)` to f\"\n",
    "#     if g is None: return (property(partial(f,i)) for i in range(n))\n",
    "#     return (property(partial(f,i), partial(g,i)) for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data block\n",
    "\n",
    "> High level API to quickly get your data in a `DataBunch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformBlock():\n",
    "    \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "    def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs=None):\n",
    "        self.type_tfms  =            L(type_tfms) # tfms for a specific type, e.g. for Category type, you need Categorize tfms\n",
    "        self.item_tfms  = ToTensor + L(item_tfms) # after_item tfms\n",
    "        self.batch_tfms =            L(batch_tfms) # after_batch tfms\n",
    "        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CategoryBlock(vocab=None, add_na=False):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    return TransformBlock(type_tfms=Categorize(vocab=vocab, add_na=add_na))\n",
    "\n",
    "\n",
    "def MultiCategoryBlock(encoded=False, vocab=None, add_na=False):\n",
    "    \"`TransformBlock` for multi-label categorical targets\"\n",
    "    tfm = EncodedMultiCategorize(vocab=vocab) if encoded else [MultiCategorize(vocab=vocab, add_na=add_na), OneHotEncode]\n",
    "    return TransformBlock(type_tfms=tfm)\n",
    "\n",
    "def RegressionBlock(c_out=None):\n",
    "    \"`TransformBlock` for float targets\"\n",
    "    return TransformBlock(type_tfms=RegressionSetup(c_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from inspect import isfunction,ismethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _merge_tfms(*tfms):\n",
    "    \"Group the `tfms` in a single list, removing duplicates (from the same class) and instantiating\"\n",
    "    g = groupby(concat(*tfms), lambda o:\n",
    "        o if isinstance(o, type) else o.__qualname__ if (isfunction(o) or ismethod(o)) else o.__class__)\n",
    "    return L(v[-1] for k,v in g.items()).map(instantiate)\n",
    "\n",
    "def _zip(x): return L(x).zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, so not exported\n",
    "from fastai2.vision.core import *\n",
    "from fastai2.vision.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = _merge_tfms([Categorize, MultiCategorize, Categorize(['dog', 'cat'])], Categorize(['a', 'b']))\n",
    "#If there are several instantiated versions, the last one is kept.\n",
    "test_eq(len(tfms), 2)\n",
    "test_eq(tfms[1].__class__, MultiCategorize)\n",
    "test_eq(tfms[0].__class__, Categorize)\n",
    "test_eq(tfms[0].vocab, ['a', 'b'])\n",
    "\n",
    "tfms = _merge_tfms([PILImage.create, PILImage.show])\n",
    "#Check methods are properly separated\n",
    "test_eq(len(tfms), 2)\n",
    "tfms = _merge_tfms([show_image, set_trace])\n",
    "#Check functions are properly separated\n",
    "test_eq(len(tfms), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@docs\n",
    "@funcs_kwargs  # look for stuff inside _methods, and determine whether some of **kwargs in __init__ will be in _methods, \n",
    "# then override the mentioned class method with it. \n",
    "class DataBlock():\n",
    "    \"Generic container to quickly build `Datasets` and `DataLoaders`\"\n",
    "    get_x=get_items=splitter=get_y = None\n",
    "    blocks,dl_type = (TransformBlock,TransformBlock),TfmdDL\n",
    "    _methods = 'get_items splitter get_y get_x'.split()\n",
    "    def __init__(self, blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None, **kwargs):\n",
    "        blocks = L(self.blocks if blocks is None else blocks)\n",
    "        blocks = L(b() if callable(b) else b for b in blocks)\n",
    "        \n",
    "        # get default type tfms\n",
    "        self.type_tfms = blocks.attrgot('type_tfms', L())\n",
    "        \n",
    "        # get the default tfms (tfms for items and batch) from the type/block\n",
    "        self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n",
    "        self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n",
    "        \n",
    "        for b in blocks: \n",
    "            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n",
    "        if dl_type is not None: self.dl_type = dl_type\n",
    "        self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)\n",
    "        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n",
    "        \n",
    "        self.getters = [noop] * len(self.type_tfms) if getters is None else getters\n",
    "        if self.get_x: self.getters[0] = self.get_x\n",
    "        if self.get_y: self.getters[1] = self.get_y\n",
    "        self.n_inp = n_inp\n",
    "        \n",
    "        if kwargs: raise TypeError(f'invalid keyword arguments: {\", \".join(kwargs.keys())}')\n",
    "        self.new(item_tfms, batch_tfms)\n",
    "        \n",
    "    def _combine_type_tfms(self): return L([self.getters, self.type_tfms]).map_zip(lambda g,tt: L(g) + tt)\n",
    "        \n",
    "    def new(self, item_tfms=None, batch_tfms=None): # merge default with extra tfms you put in __init__ such as image aug\n",
    "        self.item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n",
    "        self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n",
    "        return self\n",
    "    \n",
    "    @classmethod\n",
    "    def from_columns(cls, blocks=None, getters=None, get_items=None, **kwargs):\n",
    "        if getters is None: getters = L(ItemGetter(i) for i in range(2 if blocks is None else len(L(blocks))))\n",
    "        get_items = _zip if get_items is None else compose(get_items, _zip)\n",
    "        return cls(blocks=blocks, getters=getters, get_items=get_items, **kwargs)\n",
    "\n",
    "    def datasets(self, source, verbose=False):\n",
    "        self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n",
    "        items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n",
    "        splits = (self.splitter or RandomSplitter())(items)  \n",
    "        pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n",
    "        return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)\n",
    "\n",
    "    def dataloaders(self, source, path='.', verbose=False, **kwargs):\n",
    "        dsets = self.datasets(source)\n",
    "        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n",
    "        return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)\n",
    "\n",
    "    _docs = dict(new=\"Create a new `DataBlock` with other `item_tfms` and `batch_tfms`\",\n",
    "                 datasets=\"Create a `Datasets` object from `source`\",\n",
    "                 dataloaders=\"Create a `DataLoaders` object from `source`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??ColReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The use of DataBlock: to automatically construct a Datasets or Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a `DataBlock` you need to give the library four things: the types of your input/labels then at least two functions: `get_items` and `splitter`. You may also need to include `get_x` and `get_y` or a more generic list of `getters` that are applied to the results of `get_items`.\n",
    "\n",
    "Once those are provided, you automatically get a `Datasets` or a `DataLoaders`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DataBlock.dataloaders\" class=\"doc_header\"><code>DataBlock.dataloaders</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DataBlock.dataloaders</code>(**`source`**, **`path`**=*`'.'`*, **`verbose`**=*`False`*, **`bs`**=*`64`*, **`shuffle`**=*`False`*, **`num_workers`**=*`None`*, **`do_setup`**=*`True`*, **`pin_memory`**=*`False`*, **`timeout`**=*`0`*, **`batch_size`**=*`None`*, **`drop_last`**=*`False`*, **`indexed`**=*`None`*, **`n`**=*`None`*, **`device`**=*`None`*, **`wif`**=*`None`*, **`before_iter`**=*`None`*, **`after_item`**=*`None`*, **`before_batch`**=*`None`*, **`after_batch`**=*`None`*, **`after_iter`**=*`None`*, **`create_batches`**=*`None`*, **`create_item`**=*`None`*, **`create_batch`**=*`None`*, **`retain`**=*`None`*, **`get_idxs`**=*`None`*, **`sample`**=*`None`*, **`shuffle_fn`**=*`None`*, **`do_batch`**=*`None`*)\n",
       "\n",
       "Create a [`DataLoaders`](/data.core#DataLoaders) object from `source`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "dblock = DataBlock()\n",
    "show_doc(dblock.dataloaders, name=\"DataBlock.dataloaders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `DataBlock` by passing functions or subclassing. The two following data blocks are the same for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(DataBlock):\n",
    "    blocks = ImageBlock(cls=PILImageBW),CategoryBlock # type of X (PILImageBW), and type of y (TensorCategory)\n",
    "    # these 'blocks' contain TYPE transforms needed to CREATE and USE that specific type, \n",
    "    # this is why we have PILBase.create (with 'black and white' mode input) tfm at the end of X pipeline \n",
    "    # and Categorize tfm at the end of y pipeline\n",
    "    # remember that X and y pipeline are for DATASETS\n",
    "    \n",
    "    # these 2 blocks adds some 'default' tfms for DATALOADERS (such as ToTensor and IntToFloatTensor, see below)\n",
    "    \n",
    "    def get_items(self, source): \n",
    "        # source is the path where items are located\n",
    "        # output: get all items (for MNIST it's a list of paths)\n",
    "        # note that this is not included as a tfm or in pipeline, since we need this list first in order to do ANYTHING\n",
    "        # Note: if you return a tuple of (all Xs, all ys), then no need to define get_x, get_y \n",
    "        # (see planet example in dblock tutorial notebook. TODO: run the planet example)\n",
    "        \n",
    "        # bonus 1: get_items is useful if you have to do an extra step to get the list of items yourself\n",
    "        # if you already have that list of items, don't need to define get_items in datablock __init__\n",
    "        \n",
    "        # bonus 2: what will items of this list go? these img paths will go through x pipeline: PILBase.create \n",
    "        # for y, y pipeline is: [parent_label (see get_y), Categorize]\n",
    "        return get_image_files(Path(source))\n",
    "    \n",
    "    def splitter (self, items ): \n",
    "        # use a train/val splitter function on the list of items.\n",
    "        # return list of 2: 1 for train, 1 for val\n",
    "        return GrandparentSplitter()(items)\n",
    "    \n",
    "    def get_y    (self, item  ): \n",
    "        # input: 1 single item, from the list of items from get_items\n",
    "        # output: the label (y) of that one item \n",
    "        # if multi-label, get the list of labels, e.g.: if item is [X,y], get_y = lambda inp: inp[1].split(' '))\n",
    "        # this fn will be added to y pipeline\n",
    "        return parent_label(item)\n",
    "    \n",
    "    def get_x    (self, item  ): pass\n",
    "        # you can provide a get_x function: given 1 single item, return the X portion of that item\n",
    "        # e.g. if item is a tuple (X,y), return item[0] aka X\n",
    "mnist = MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent to\n",
    "mnist = DataBlock(blocks = (ImageBlock(cls=PILImageBW),CategoryBlock),\n",
    "                  get_items = get_image_files,\n",
    "                  splitter = GrandparentSplitter(),\n",
    "                  get_y = parent_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each type comes with default transforms that will be applied\n",
    "- at the base level to create items in a tuple (usually input,target) from the base elements (like filenames)\n",
    "- at the item level of the datasource\n",
    "- at the batch level\n",
    "\n",
    "They are called respectively type transforms, item transforms, batch transforms. In the case of MNIST, the type transforms are the method to create a `PILImageBW` (for the input) and the `Categorize` transform (for the target), the item transform is `ToTensor` and the batch transforms are `Cuda` and `IntToFloatTensor`. You can add any other transforms by passing them in `DataBlock.datasets` or `DataBlock.dataloaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??PILBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [<bound method PILBase.create of <class 'fastai2.vision.core.PILImageBW'>>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.type_tfms[0] # type tfm for X pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorize: (object,object) -> encodes (object,object) -> decodes"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.type_tfms[1][0] # type tfm for y pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai2.data.transforms.Categorize"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.type_tfms[1][0]) # Categorize (class inherit from Transform), will be included in y pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mnist.type_tfms[0], [PILImageBW.create])\n",
    "test_eq(mnist.type_tfms[1].map(type), [Categorize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mnist.default_item_tfms.map(type), [ToTensor])\n",
    "test_eq(mnist.default_batch_tfms.map(type), [IntToFloatTensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTensor: (PILMask,object) -> encodes\n",
      "(PILBase,object) -> encodes \n",
      "IntToFloatTensor: (TensorMask,object) -> encodes\n",
      "(TensorImage,object) -> encodes (TensorImage,object) -> decodes\n"
     ]
    }
   ],
   "source": [
    "print(mnist.default_item_tfms[0]) #default item tfm (for dataloaders' after_item pipeline) for PILBase: ToTensor (2 encodes)\n",
    "\n",
    "print(mnist.default_batch_tfms[0]) # default batch item tfm (for dataloaders' after_batch pipeline): IntToFloatTensor (2 encodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTensor: (PILMask,object) -> encodes\n",
      "(PILBase,object) -> encodes \n",
      "IntToFloatTensor: (TensorMask,object) -> encodes\n",
      "(TensorImage,object) -> encodes (TensorImage,object) -> decodes\n"
     ]
    }
   ],
   "source": [
    "#same as above, since we don't have extra item_tfms or batch_tfms in datablock __init__. If there are, they will be in here\n",
    "#see def new to see why\n",
    "print(mnist.item_tfms[0])\n",
    "print(mnist.batch_tfms[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can create datasets or dataloaders using datablock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/quantran/.fastai/data/mnist_tiny')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = mnist.datasets(untar_data(URLs.MNIST_TINY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709, 699)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsets.train), len(dsets.valid) # use GrandParent splitter to split train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline: PILBase.create, Pipeline: parent_label -> Categorize)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.tls[0].tfms,dsets.tls[1].tfms # X and y pipelines transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAACLCAYAAABBVeZmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAADpklEQVR4nO3dP0tbURjH8eeIgtiCOog1Q0UoKA4OjhUXJ19AQbI0Q96Jb6GvoN2k0MVJOkkzCrqI+AcyqGCLi7SDSOrpVOlzWpO0JPfc9Pv7QMGHtOaUb8+9JjdJQ4zRhGEo9wKkOIoNotggig2i2CCKDaLYIKjYIYRvya/vIYQ3uddVlOHcCyhSjPHpz69DCE/M7LOZvc+3omKhdnbilZl9MbNPuRdSFHLsmpm9i6DniwPo7/oghPDczJpm9iLG2My9nqJQd/ZrM2uQQpuxY7/NvYii4Q7jIYSXZvbRzJ7FGL/mXk+RiDu7ZmYfaKHNgDubjLizsRQbRLFBFBtEsUE6XfXSj+qDJzx2g3Y2iGKDKDaIYoMoNohigyg2iGKDKDaIYoMoNohigyg2iGKDKDaIYoMoNohigyg2iGKDKDaIYoMoNohigyg2iGKDlPZD7w4ODtx8fHzs5oWFBTdvb2+7+fr62s2/fujA7u6uu215ednN9XrdzSsrK12suPy0s0EUG0SxQTp9gE62t+xubW25uVqttv39w8P+x4+1tTU339/fP3w9NOT/jZ+cnLj55ubGzZeXl24eHR1tu5bM9JZdUWwUxQYp7ePs29vbtrcfHh66eWZmxs0TExNd39f5+bmbZ2dn3Zye05eWlrr+3mWinQ2i2CCKDVLac/b+/n7b26enp938N+fo1NjY2D//2UGinQ2i2CCKDaLYIIoNotggig1S2sfZ6XX2yclJN4+MjPTsvtLXu01NTbl5UJ8LT2lngyg2iGKDlPY1aHd3d25OXwdeqVR6dl+1Ws3NOzs7br66uurZfRVAr0ETxUYp7WG8n1qtlpsXFxfdHII/EqZvPSo5HcZFsVEUG6S0T5f208XFhZvPzs7cvLm5WeRyCqOdDaLYIIoNotggig2i2CCKDYJ8nJ1Krw90uF4wsLSzQRQbRLFBdM62369fr66uZlpJf2lngyg2iGKD6Jz9B3Nzc7mX0Bfa2SCKDaLYIMhzdrPZzL2ELLSzQRQbRLFBkOfsRqORewlZaGeDKDaIYoMgz9mU15yltLNBFBtEsUGQn6lyenrq5vn5eTenz52n//VTyekzVUSxUZAPvTp9ovHR0ZGbB+ww/ijtbBDFBlFsEOQ5u5O9vT03r6+vZ1pJb2lngyg2iGKD6JxtZuPj427e2NjItJL+0s4GUWwQxQZBXuL8z+kSpyg2imKDdHqc/ejxXwaPdjaIYoMoNohigyg2iGKD/ABxBbbC7YTxmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_eq(dsets.vocab, ['3', '7'])\n",
    "x,y = dsets.train[0]\n",
    "test_eq(x.size,(28,28))\n",
    "show_at(dsets.train, 0, cmap='Greys', figsize=(2,2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fastai2.vision.core.PILImageBW, 'torch.LongTensor')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x),y.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fastai2.torch_core.TensorCategory, 'torch.LongTensor')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y),y.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not do one pass in your dataloader, there is something wrong in it\n"
     ]
    }
   ],
   "source": [
    "dbunch = mnist.dataloaders(untar_data(URLs.MNIST_TINY)) # TODO: bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
