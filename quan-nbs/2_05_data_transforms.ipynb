{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few lines of the module defines the classes and the functionalities that needs to be imported.\n",
    "\n",
    "```python \n",
    "__all__ =['get_files', 'FileGetter', 'image_extensions' ...] #note that this will be created automatically in fastai\n",
    "``` \n",
    "\n",
    "When we specify from fastai.core.module import * , **python will import only functions and classes in __all__**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.torch_basics import *\n",
    "from fastai2.data.core import *\n",
    "from fastai2.data.load import *\n",
    "from fastai2.data.external import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TensorCategory(TensorBase): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for processing data and basic transforms\n",
    "\n",
    "> Functions for getting, splitting, and labeling data, as well as generic transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get, split, and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most data source creation we need functions to get a list of items, split them in to train/valid sets, and label them. fastai provides functions to make each of these steps easy (especially when combined with `fastai.data.blocks`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll look at functions that *get* a list of items (generally file names).\n",
    "\n",
    "We'll use *tiny MNIST* (a subset of MNIST with just two classes, `7`s and `3`s) for our examples/tests throughout this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [/home/quantran/.fastai/data/mnist_tiny/train/7,/home/quantran/.fastai/data/mnist_tiny/train/3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)\n",
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/quantran/.fastai/data/mnist_tiny')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/quantran/.fastai/data/mnist_tiny/test\n",
      "/home/quantran/.fastai/data/mnist_tiny/models\n",
      "/home/quantran/.fastai/data/mnist_tiny/labels.csv\n",
      "/home/quantran/.fastai/data/mnist_tiny/valid\n",
      "/home/quantran/.fastai/data/mnist_tiny/train\n"
     ]
    }
   ],
   "source": [
    "for i in path.ls(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# note that anything with _ in front (_get_files) is not included in __all__ => not get exported using *\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_files(path, extensions=None, recurse=True, folders=None):\n",
    "    \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n",
    "    path = Path(path)\n",
    "    folders=L(folders)\n",
    "    extensions = setify(extensions)\n",
    "    extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames). os.walk is fucking fast!\n",
    "            if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders]\n",
    "            else:                         d[:] = [o for o in d if not o.startswith('.')]\n",
    "            res += _get_files(p, f, extensions)\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        res = _get_files(path, f, extensions)\n",
    "    return L(res)\n",
    "# this is the best get_files function there is (tested on 1.3 mil imagenet files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most general way to grab a bunch of file names from disk. If you pass `extensions` (including the `.`) then returned file names are filtered by that list. Only those files directly in `path` are included, unless you pass `recurse`, in which case all child folders are also searched recursively. `folders` is an optional list of directories to limit the search to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = get_files(path/'train'/'3', extensions='.png', recurse=False)\n",
    "t7 = get_files(path/'train'/'7', extensions='.png', recurse=False)\n",
    "t  = get_files(path/'train', extensions='.png', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [/home/quantran/.fastai/data/mnist_tiny/train/7,/home/quantran/.fastai/data/mnist_tiny/train/3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#0) []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = get_files(path/'train', extensions='.png', recurse=False) # no recursive = not going into directory to look for files with given extensions\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#709) [/home/quantran/.fastai/data/mnist_tiny/train/7/9286.png,/home/quantran/.fastai/data/mnist_tiny/train/7/7686.png,/home/quantran/.fastai/data/mnist_tiny/train/7/8137.png,/home/quantran/.fastai/data/mnist_tiny/train/7/79.png,/home/quantran/.fastai/data/mnist_tiny/train/7/8198.png,/home/quantran/.fastai/data/mnist_tiny/train/7/7663.png,/home/quantran/.fastai/data/mnist_tiny/train/7/9433.png,/home/quantran/.fastai/data/mnist_tiny/train/7/8502.png,/home/quantran/.fastai/data/mnist_tiny/train/7/9270.png,/home/quantran/.fastai/data/mnist_tiny/train/7/7380.png...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eq(len(t), len(t3)+len(t7))\n",
    "test_eq(len(get_files(path/'train'/'3', extensions='.jpg', recurse=False)),0)\n",
    "test_eq(len(t), len(get_files(path, extensions='.png', recurse=True, folders='train')))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(len(get_files(path/'train'/'3', recurse=False)),346)\n",
    "test_eq(len(get_files(path, extensions='.png', recurse=True, folders=['train', 'test'])),729)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to be able to create functions with customized behavior. `fastai.data` generally uses functions named as CamelCase verbs ending in `er` to create these functions. `FileGetter` is a simple example of such a function creator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def FileGetter(suf='', extensions=None, recurse=True, folders=None):\n",
    "    \"Create `get_files` partial function that searches path suffix `suf`, only in `folders`, if specified, and passes along args\"\n",
    "    def _inner(o, extensions=extensions, recurse=recurse, folders=folders):\n",
    "        return get_files(o/suf, extensions, recurse, folders)\n",
    "    return _inner\n",
    "# function to return a function (similar to partial function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpng = FileGetter(suf='train/7', extensions='.png', recurse=False)\n",
    "test_eq(len(t7), len(fpng(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as above\n",
    "fpng = FileGetter(extensions='.png', recurse=False)\n",
    "test_eq(len(t7), len(fpng(path/'train'/'7')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(t), len(fpng(path/'train', recurse=True)))\n",
    "\n",
    "#same as above\n",
    "fpng_r = FileGetter(extensions='.png', recurse=True)\n",
    "test_eq(len(t), len(fpng_r(path/'train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_image_files(path, recurse=True, folders=None):\n",
    "    \"Get image files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=image_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply `get_files` called with a list of standard image extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(t), len(get_image_files(path, recurse=True, folders='train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ImageGetter(suf='', recurse=True, folders=None):\n",
    "    \"Create `get_image_files` partial function that searches path suffix `suf` and passes along `kwargs`, only in `folders`, if specified.\"\n",
    "    def _inner(o, recurse=recurse, folders=folders): return get_image_files(o/suf, recurse, folders)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `FileGetter`, but for image extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(get_files(path/'train', extensions='.png', recurse=True, folders='3')),\n",
    "        len(ImageGetter(   'train',                    recurse=True, folders='3')(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_text_files(path, recurse=True, folders=None):\n",
    "    \"Get text files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=['.txt'], recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of functions are used to *split* data into training and validation sets. The functions return two lists - a list of indices or masks for each of training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def RandomSplitter(valid_pct=0.2, seed=None, **kwargs):\n",
    "    \"Create function that splits `items` between train/val with `valid_pct` randomly.\"\n",
    "    def _inner(o, **kwargs):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        rand_idx = L(int(i) for i in torch.randperm(len(o)))\n",
    "        cut = int(valid_pct * len(o))\n",
    "        return rand_idx[cut:],rand_idx[:cut]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = list(range(30))\n",
    "f = RandomSplitter(seed=42)\n",
    "trn,val = f(src)\n",
    "assert 0<len(trn)<len(src)\n",
    "assert all(o not in val for o in trn)\n",
    "test_eq(len(trn), len(src)-len(val))\n",
    "# test random seed consistency\n",
    "test_eq(f(src)[0], trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grandparent splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _grandparent_idxs(items, name): \n",
    "    # return idx of img path that has parents == name (input parameter)\n",
    "    return mask2idxs(Path(o).parent.parent.name == name for o in items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [path/'train/3/9932.png', path/'valid/7/7189.png', \n",
    "         path/'valid/7/7320.png', path/'train/7/9833.png',  \n",
    "         path/'train/3/7666.png', path/'valid/3/925.png',\n",
    "         path/'train/7/724.png', path/'valid/3/93055.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False, True, True, False, True, False]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Path(o).parent.parent.name == 'train' for o in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 4, 6]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask2idxs([Path(o).parent.parent.name == 'train' for o in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def GrandparentSplitter(train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the grand parent folder names (`train_name` and `valid_name`).\"\n",
    "    def _inner(o, **kwargs):\n",
    "        return _grandparent_idxs(o, train_name),_grandparent_idxs(o, valid_name)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GrandparentSplitter()\n",
    "test_eq(splitter(items),[[0,3,4,6],[1,2,5,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def IndexSplitter(valid_idx): #specific file indices for validation set\n",
    "    \"Split `items` so that `val_idx` are in the validation set and the others in the training set\"\n",
    "    def _inner(o, **kwargs):\n",
    "        train_idx = np.setdiff1d(np.array(range_of(o)), np.array(valid_idx)) # find differences of 2 arrays (to be train_idx)\n",
    "        return L(train_idx, use_list=True), L(valid_idx, use_list=True)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(range(10))\n",
    "splitter = IndexSplitter([3,7,9])\n",
    "test_eq(splitter(items),[[0,1,2,4,5,6,8],[3,7,9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom func splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def FuncSplitter(func): # func will return either true or false for EACH file (only one input at a time, no loop); true files' idxs will be val_idx\n",
    "    \"Split `items` by result of `func` (`True` for validation, `False` for training set).\"\n",
    "    def _inner(o, **kwargs):\n",
    "        val_idx = mask2idxs(func(o_) for o_ in o)\n",
    "        return IndexSplitter(val_idx)(o)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [path/'train/3/9932.png', path/'valid/7/7189.png', \n",
    "          path/'valid/7/7320.png', path/'train/7/9833.png',  \n",
    "          path/'train/3/7666.png', path/'valid/3/925.png',\n",
    "          path/'train/7/724.png', path/'valid/3/93055.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = FuncSplitter(lambda o: Path(o).parent.parent.name == 'valid')\n",
    "test_eq(splitter(fnames),[[0,3,4,6],[1,2,5,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskSplitter(mask): # True values in mask will go into val_idx\n",
    "    \"Split `items` depending on the value of `mask`.\"\n",
    "    def _inner(o, **kwargs): return IndexSplitter(mask2idxs(mask))(o)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(range(6))\n",
    "splitter = MaskSplitter([True,False,False,True,False,True])\n",
    "test_eq(splitter(items),[[1,2,4],[0,3,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FileSplitter(fname): # read a txt file and save indices of all files in that txt file as val_idx\n",
    "    valid = Path(fname).read().split('\\n') \n",
    "    def _func(x): return x.name in valid\n",
    "    def _inner(o, **kwargs): return FuncSplitter(_func)(o)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    fname = Path(d)/'valid.txt'\n",
    "    fname.write('\\n'.join([Path(fnames[i]).name for i in [1,3,4]]))\n",
    "    splitter = FileSplitter(fname)\n",
    "    test_eq(splitter(fnames),[[0,2,5,6,7],[1,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/tmp/tmp0wbs1_h9/valid.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7189.png\\n9833.png\\n7666.png'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.join([Path(fnames[i]).name for i in [1,3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Col splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def ColSplitter(col='is_valid'): # there is a boolean 'valid' column in dataframe to signal which row will be in val\n",
    "    \"Split `items` (supposed to be a dataframe) by value in `col`\"\n",
    "    def _inner(o, **kwargs):\n",
    "        assert isinstance(o, pd.DataFrame), \"ColSplitter only works when your items are a pandas DataFrame\"\n",
    "        valid_idx = o[col].values\n",
    "        return IndexSplitter(mask2idxs(valid_idx))(o)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': [0,1,2,3,4], 'b': [True,False,True,True,False]})\n",
    "splits = ColSplitter('b')(df)\n",
    "test_eq(splits, [[1,4], [0,2,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final set of functions is used to *label* a single item of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parent_label(o, **kwargs): # no need for capital letter function since there's no need for customization through input parameters\n",
    "    \"Label `item` with the parent folder name.\"\n",
    "    return Path(o).parent.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `parent_label` doesn't have anything customize, so it doesn't return a function - you can just use it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/3/9932.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/valid/7/7189.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/valid/7/7320.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/7/9833.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/3/7666.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/valid/3/925.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/7/724.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/valid/3/93055.png')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '7', '7', '3', '3', '7', '3']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eq(parent_label(fnames[0]), '3')\n",
    "test_eq(parent_label(\"whatever/the/hell/3/666.png\"), '3')\n",
    "[parent_label(o) for o in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RegexLabeller():\n",
    "    \"Label `item` with regex `pat`.\"\n",
    "    def __init__(self, pat): self.pat = re.compile(pat)\n",
    "        \n",
    "    def __call__(self, o, **kwargs):\n",
    "        res = self.pat.search(str(o))\n",
    "        assert res,f'Failed to find \"{self.pat}\" in \"{o}\"'\n",
    "        return res.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RegexLabeller` is a very flexible function since it handles any regex search of the stringified item. For instance, here's an example the replicates the previous `parent_label` results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '7', '7', '3', '3', '7', '3']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = RegexLabeller(r'/(\\d)/')\n",
    "[f(o) for o in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '7', '7', '3', '3', '7', '3']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp = re.compile(f'{re.escape(os.path.sep)}(\\d){re.escape(os.path.sep)}')\n",
    "f = RegexLabeller(regexp)\n",
    "test_eq(parent_label(fnames[0]), '3')\n",
    "[f(o) for o in fnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColReader(): # TODO: wtf is this\n",
    "    \"Read `cols` in `row` with potential `pref` and `suff`\"\n",
    "    def __init__(self, cols, pref='', suff='', label_delim=None):\n",
    "        store_attr(self, 'suff,label_delim')\n",
    "        self.pref = str(pref) + os.path.sep if isinstance(pref, Path) else pref\n",
    "        self.cols = L(cols)\n",
    "    \n",
    "    def _do_one(self, r, c):\n",
    "        o = r[c] if isinstance(c, int) else getattr(r, c)\n",
    "        if len(self.pref)==0 and len(self.suff)==0 and self.label_delim is None: return o\n",
    "        if self.label_delim is None: return f'{self.pref}{o}{self.suff}'\n",
    "        else: return o.split(self.label_delim) if len(o)>0 else []\n",
    "    \n",
    "    def __call__(self, o, **kwargs): return detuplify(tuple(self._do_one(o, c) for c in self.cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cols can be a list of column names or a list of indices (or a mix of both). If label_delim is passed, the result is split using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': 'a b c d'.split(), 'b': ['1 2', '0', '', '1 2 3']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>1 2 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a      b\n",
       "0  a    1 2\n",
       "1  b      0\n",
       "2  c       \n",
       "3  d  1 2 3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pandas(Index=0, a='a', b='1 2'),\n",
       " Pandas(Index=1, a='b', b='0'),\n",
       " Pandas(Index=2, a='c', b=''),\n",
       " Pandas(Index=3, a='d', b='1 2 3')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o for o in df.itertuples()] # loop each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 2', '0', '', '1 2 3']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract values of column b into arrays\n",
    "f = ColReader('b')\n",
    "[f(o) for o in df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2'], ['0'], [], ['1', '2', '3']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract column b into arrays, split into subarrays using white space\n",
    "f = ColReader('b', label_delim=' ')\n",
    "[f(o) for o in df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', '1 2'), ('b', '0'), ('c', ''), ('d', '1 2 3')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = ColReader(['a','b'])\n",
    "[f(o) for o in df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wtf is this\n",
    "f = ColReader('a', pref='0', suff='1')\n",
    "test_eq([f(o) for o in df.itertuples()], '0a1 0b1 0c1 0d1'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 6, 7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           a\n",
       "0     [0, 1]\n",
       "1  [2, 3, 4]\n",
       "2  [5, 6, 7]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'a': [L(0,1), L(2,3,4), L(5,6,7)]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(#2) [0,1], (#3) [2,3,4], (#3) [5,6,7]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = ColReader('a')\n",
    "[f(o) for o in df.itertuples()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CategoryMap(CollBase):\n",
    "    \"Collection of categories with the reverse mapping in `o2i`\"\n",
    "    def __init__(self, col, sort=True, add_na=False):\n",
    "        if is_categorical_dtype(col): items = L(col.cat.categories, use_list=True)\n",
    "        else:\n",
    "            if not hasattr(col,'unique'): col = L(col, use_list=True)\n",
    "            # `o==o` is the generalized definition of non-NaN used by Pandas\n",
    "            items = L(o for o in col.unique() if o==o)\n",
    "            if sort: items = items.sorted()\n",
    "        self.items = '#na#' + items if add_na else items # add NaN category/label, e.g. for categorical feature in tabular\n",
    "        self.o2i = defaultdict(int, self.items.val2idx()) if add_na else dict(self.items.val2idx())\n",
    "    def __eq__(self,b): return all_equal(b,self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = CategoryMap([4,2,3,4])\n",
    "test_eq(t, [2,3,4])\n",
    "test_eq(t.o2i, {2:0,3:1,4:2})\n",
    "test_fail(lambda: t.o2i['unseen label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [2,3,4]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 0, 2: 1, 3: 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = L([4,2,3])\n",
    "t.val2idx() # <L-instance>.val2idx(), make sure t contains unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t == t.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#4) [#na#,2,3,4]\n",
      "defaultdict(<class 'int'>, {'#na#': 0, 2: 1, 3: 2, 4: 3})\n"
     ]
    }
   ],
   "source": [
    "t = CategoryMap([4,2,3,4], add_na=True)\n",
    "print(t)\n",
    "test_eq(t, ['#na#',2,3,4])\n",
    "test_eq(t.o2i, {'#na#':0,2:1,3:2,4:3})\n",
    "print(t.o2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = CategoryMap(pd.Series([4,2,3,4]), sort=False)\n",
    "test_eq(t, [4,2,3])\n",
    "test_eq(t.o2i, {4:0,2:1,3:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pd.Series(pd.Categorical(['M','H','L','M'], categories=['H','M','L'], ordered=True))\n",
    "t = CategoryMap(col)\n",
    "test_eq(t, ['H','M','L'])\n",
    "test_eq(t.o2i, {'H':0,'M':1,'L':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [[1],[2, 3]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=L([1],[2,3])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], list)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t,[]),type(sum(t,[])) #wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [[1],[2, 3]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different from sum(t,[]). Also not convert to list\n",
    "t+[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Category(str, ShowTitle): _show_args = {'label': 'category'} #string base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Categorize(Transform):\n",
    "    \"Reversible transform of category string to `vocab` id\"\n",
    "    loss_func,order=CrossEntropyLossFlat(),1\n",
    "    def __init__(self, vocab=None, add_na=False):\n",
    "        self.add_na = add_na\n",
    "        self.vocab = None if vocab is None else CategoryMap(vocab, add_na=add_na)\n",
    "\n",
    "    def setups(self, dsrc):\n",
    "        if self.vocab is None and dsrc is not None: \n",
    "            self.vocab = CategoryMap(dsrc, add_na=self.add_na)\n",
    "            \n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, o): \n",
    "        return TensorCategory(self.vocab.o2i[o]) \n",
    "        #vocab is CategoryMap, containing sorted labels (categories) and label2index (o2i)\n",
    "        # return type is TensorCategory, a TensorBase (TODO: why?)\n",
    "        \n",
    "    def decodes(self, o): return Category(self.vocab[o])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??TensorCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Categorize()\n",
    "# This will call 'setups' function in Categorize. Then we don't really use DataSource for the rest of this code\n",
    "tds = DataSource(['cat', 'dog', 'cat'], tfms=[cat]) # tfms = [Categorize()]. \n",
    "test_eq(cat.vocab, ['cat', 'dog'])\n",
    "test_eq(cat('cat'), 0) # encode\n",
    "test_eq(cat.decode(1), 'dog')\n",
    "test_stdout(lambda: show_at(tds,2), 'cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai2.torch_core.TensorCategory"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cat('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Categorize(add_na=True)\n",
    "tds = DataSource(['cat', 'dog', 'cat'], tfms=[cat])\n",
    "test_eq(cat.vocab, ['#na#', 'cat', 'dog'])\n",
    "test_eq(cat('cat'), 1)\n",
    "test_eq(cat('#na#'), 0)\n",
    "test_eq(cat.decode(2), 'dog')\n",
    "test_eq(cat.decode(0), '#na#')\n",
    "test_stdout(lambda: show_at(tds,2), 'cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicategorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiCategory(L):\n",
    "    def show(self, ctx=None, sep=';', color='black', **kwargs):\n",
    "        return show_title(sep.join(self.map(str)), ctx=ctx, color=color, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MultiCategorize(Categorize):\n",
    "    \"Reversible transform of multi-category strings to `vocab` id\"\n",
    "    loss_func,order=BCEWithLogitsLossFlat(),1\n",
    "    def __init__(self, vocab=None, add_na=False):\n",
    "        self.add_na = add_na\n",
    "        self.vocab = None if vocab is None else CategoryMap(vocab, add_na=add_na)\n",
    "        \n",
    "    def setups(self, dsrc):\n",
    "        if not dsrc: return\n",
    "        if self.vocab is None:\n",
    "            vals = set()\n",
    "            for b in dsrc: vals = vals.union(set(b)) # turn the mess into 1 set\n",
    "            self.vocab = CategoryMap(list(vals), add_na=self.add_na)\n",
    "\n",
    "    def encodes(self, o): return TensorMultiCategory([self.vocab.o2i[o_] for o_ in o]) # this is TensorCategory based, which is TensorBase\n",
    "    def decodes(self, o): return MultiCategory      ([self.vocab    [o_] for o_ in o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??TensorMultiCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = MultiCategorize()\n",
    "# This will call 'setups' function. Then we don't really use DataSource for the rest of this code\n",
    "tds = DataSource([['b', 'c'], ['a'], ['a', 'c']], tfms=[cat])\n",
    "\n",
    "test_eq(cat.vocab, ['a', 'b', 'c'])\n",
    "test_eq(cat(['a', 'c']), tensor([0,2]))\n",
    "test_eq(cat([]), tensor([]))\n",
    "test_eq(cat.decode([1]), ['b'])\n",
    "test_eq(cat.decode([0,2]), ['a', 'c'])\n",
    "test_stdout(lambda: show_at(tds,2), 'a;c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot1(x, c):\n",
    "    \"One-hot encode `x` with `c` classes.\"\n",
    "    res = torch.zeros(c, dtype=torch.bool)\n",
    "    res[L(x, use_list=None)] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot1(1,c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot1(0,c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot1([0,2],c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class OneHotEncode(Transform):\n",
    "    \"One-hot encodes targets\"\n",
    "    order=2\n",
    "    def __init__(self, c=None): self.c = c\n",
    "\n",
    "    def setups(self, dsrc):\n",
    "        if self.c is None: self.c = len(L(getattr(dsrc, 'vocab', None)))\n",
    "        if not self.c: warn(\"Couldn't infer the number of classes, please pass a value for `c` at init\")\n",
    "\n",
    "    def encodes(self, o): return TensorMultiCategory(one_hot(o, self.c).float()) # onehot data, then convert boolean to TensorMultiCategory as output\n",
    "    def decodes(self, o): return one_hot_decode(o, None)\n",
    "    \n",
    "# class OneHotEncode(Transform):\n",
    "#     \"One-hot encodes targets and optionally decodes with `vocab`\"\n",
    "#     order=2\n",
    "#     def __init__(self, do_encode=True, vocab=None): self.do_encode,self.vocab = do_encode,vocab\n",
    "\n",
    "#     def setups(self, dsrc):\n",
    "#         if self.vocab is not None:  self.c = len(self.vocab)\n",
    "#         else: self.c = len(L(getattr(dsrc, 'vocab', None)))\n",
    "#         if not self.c: warn(\"Couldn't infer the number of classes, please pass a `vocab` at init\")\n",
    "    \n",
    "#     def encodes(self, o): return one_hot(o, self.c) if self.do_encode else tensor(o).byte() # note that this will return a tensor\n",
    "#     def decodes(self, o): return one_hot_decode(o, self.vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works in conjunction with ` MultiCategorize` or on its own if you have one-hot encoded targets (pass a `vocab` for decoding and `do_encode=False` in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorBase??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tfm = OneHotEncode(c=3)\n",
    "test_eq(_tfm([0,2]), tensor([1.,0,1.])) # has to be float tensor, not int or long like [1,0,1] since TensorBase is float\n",
    "test_eq(_tfm.decode(tensor([0,1,1])), [1,2])\n",
    "test_eq(_tfm.decode(tensor([False,True,True])), [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 1.]), fastai2.torch_core.TensorMultiCategory)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tfm([0,2]),type(_tfm([0,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _tfm = OneHotEncode(vocab=['a', 'b', 'c'])\n",
    "# tds = DataSource([[1,2], [0], [0, 1]], tfms=[_tfm]) \n",
    "# # you don't put str label ('a','b'...) here since this tfm should be after multicategorize, where str are numericalized (See below)\n",
    "# # in that case, output of multicategorize's vocab will be used for OneHotEncode setups: self.c = len(L(getattr(dsrc, 'vocab', None)))\n",
    "# test_eq(_tfm([0,2]), tensor([1, 0, 1]).byte())\n",
    "# test_eq(_tfm.decode(tensor([0,1,1])), ['b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _tfm(1), _tfm([0,2]) # calling encode in OneHotEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do_encode = False => you have to input onehot-ed data (OneHotEncode won't one_hot for you)\n",
    "# _tfm = OneHotEncode(vocab=['a', 'b', 'c'], do_encode=False)\n",
    "# tds = DataSource([[0,1,1], [1,0,0], [1,1,0]], tfms=[_tfm])\n",
    "# test_eq(_tfm([1,0,1]), tensor([1, 0, 1]).byte())\n",
    "# test_eq(_tfm.decode(tensor([0,1,1])), ['b','c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine MultiCategorize and OneHotEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = DataSource([['b', 'c'], ['a'], ['a', 'c'], []], [[MultiCategorize(), OneHotEncode()]])\n",
    "# Note that data in DataSource ALREADY numericalized by multicat and then 1hotencoded to be TensorMultiCategory\n",
    "test_eq(tds[1], [tensor([1.,0,0])]) \n",
    "test_eq(tds[3], [tensor([0.,0,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded by Onehot, then decoded by MultiCat to spit out string labels. Note that none of OneHot or MultiCat has show() func so it decodes ALL THE WAY BACK\n",
    "test_eq(tds.decode([tensor([False, True, True])]), [['b','c']]) \n",
    "\n",
    "test_eq(type(tds[1][0]), TensorMultiCategory)\n",
    "test_stdout(lambda: show_at(tds,2), 'a;c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [(tensor([0., 1., 1.]),),(tensor([1., 0., 0.]),),(tensor([1., 0., 1.]),),(tensor([0., 0., 0.]),)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data in tds are already numericalized by multicat and then 1hotencoded to be TensorMultiCategory\n",
    "tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#4) [(tensor([1., 0., 0.]),),(tensor([0., 1., 0.]),),(tensor([1., 0., 0.]),),(tensor([0., 0., 1.]),)]\n"
     ]
    }
   ],
   "source": [
    "# you can use MultiCat even for single label dataset\n",
    "tds = DataSource(['a','b','a','c'], [[MultiCategorize(), OneHotEncode()]])\n",
    "print(tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tds[1], [tensor([0,1.,0])])\n",
    "test_eq(tds.decode([tensor([False, True, False])]), [['b']])\n",
    "test_eq(type(tds[1][0]), TensorMultiCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with passing the vocab\n",
    "tds = DataSource([['b', 'c'], ['a'], ['a', 'c'], []], [[MultiCategorize(vocab=['a', 'b', 'c']), OneHotEncode()]])\n",
    "test_eq(tds[1], [tensor([1.,0,0])])\n",
    "test_eq(tds[3], [tensor([0.,0,0])])\n",
    "test_eq(tds.decode([tensor([False, True, True])]), [['b','c']])\n",
    "test_eq(type(tds[1][0]), TensorMultiCategory)\n",
    "test_stdout(lambda: show_at(tds,2), 'a;c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EncodedMultiCategorize(Categorize):\n",
    "    \"Transform of one-hot encoded multi-category that decodes with `vocab`\"\n",
    "    loss_func,order=BCEWithLogitsLossFlat(),1\n",
    "    def __init__(self, vocab): self.vocab,self.c = vocab,len(vocab)\n",
    "    def encodes(self, o): return TensorCategory(tensor(o).float()) # accept onehot-encoded data?\n",
    "    def decodes(self, o): return MultiCategory (one_hot_decode(o, self.vocab)) # decode onehot back to string label using vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tfm = EncodedMultiCategorize(vocab=['a', 'b', 'c'])\n",
    "test_eq(_tfm([1,0,1]), tensor([1., 0., 1.]))\n",
    "test_eq(type(_tfm([1,0,1])), TensorCategory)\n",
    "test_eq(_tfm.decode(tensor([True, False, True])), ['a','c'])\n",
    "test_eq(_tfm.decode(tensor([1., 0, 1])), ['a','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_c(dbunch): # get number of classes (labels)\n",
    "    if getattr(dbunch, 'c', False): return dbunch.c\n",
    "    vocab = getattr(dbunch, 'vocab', [])\n",
    "    if len(vocab) > 0 and is_listy(vocab[-1]): vocab = vocab[-1]\n",
    "    return len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end dataset example with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show how to use those functions to grab the mnist dataset in a `DataSource`. First we grab all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = get_image_files(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split between train and validation depending on the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GrandparentSplitter() \n",
    "splits = splitter(items) # [<indices of train img>,<indices of val img>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#3) [/home/quantran/.fastai/data/mnist_tiny/train/7/9286.png,/home/quantran/.fastai/data/mnist_tiny/train/7/7686.png,/home/quantran/.fastai/data/mnist_tiny/train/7/8137.png],\n",
       " (#3) [/home/quantran/.fastai/data/mnist_tiny/valid/7/9851.png,/home/quantran/.fastai/data/mnist_tiny/valid/7/8510.png,/home/quantran/.fastai/data/mnist_tiny/valid/7/946.png])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,valid = (items[i] for i in splits)\n",
    "train[:3],valid[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inputs are images that we open and convert to tensors, our targets are labeled depending on the parent directory and are categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def open_img(fn:Path): return Image.open(fn).copy() # doesn't use PILImage.create because haven't reached that part yet in this notebook\n",
    "def img2tensor(im:Image.Image): return TensorImage(array(im)[None])\n",
    "# note that these 2 are just functions, but when put into pipeline, they become 'Transform' (see more in live-code notebook)\n",
    "\n",
    "tfms = [[open_img, img2tensor],\n",
    "        [parent_label, Categorize()]]\n",
    "train_ds = DataSource(train, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/7/79.png')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_ds[3]\n",
    "xd,yd = decode_at(train_ds,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]),\n",
       " torch.Size([1, 28, 28]),\n",
       " fastai2.torch_core.TensorImage,\n",
       " fastai2.torch_core.TensorImage)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,xd.shape,type(x),type(xd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(x,xd) \n",
    "# decode does not do anything, because output of img2Tensor is TensorImage, which knows how to show(). Also img2tensor and open_img does not have 'decode' func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcdef7fa7d0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), '7', fastai2.torch_core.TensorCategory, __main__.Category)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,yd,type(y),type(yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(parent_label(train[3]),yd)\n",
    "test_eq(array(Image.open(train[3])),xd[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??show_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABUCAYAAAA7xZEpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADuElEQVR4nO2bPUgcURSFvyeKYgKihYiFIkRBZAtBRWJhY2ElCOkUo7C1P5W1lQg2ktIqUbEICNoKWhgLfxoVEbSw0GZtFJVFBJkUm5fJXjfZ0czsm4T7wcIy7sw7nD1z993rrvE8D8WnyLWAuKGGCNQQgRoiUEMEaohADRE4N8QYcy8eT8aYT670FLta2OJ53lv73BjzBkgBX13pcZ4QwQfgCthyJSBuhnwEvngO+wkTl17GGFMHnAPvPM87d6UjTgkZAr65NAPiZ8hn1yJiccsYY94D60CN53l3LrXEJSEfgRXXZkBMEhIn4pKQ2KCGCNQQgRoiyNfc/c8V1+Q6qAkRqCECNUSghgjUEIEaIlBDBGqIQA0RqCECNUSghgjUEIHzf2Xm4+bmBoClpSUARkdHAejr6wNgeXkZgLKyslDW04QI8g2ZCzIPSafT7OzsADA2NpZZ+Ieu+/t7AC4uLrKF/fj7xMQEALOzsy9dVuchQShoDXl6egL8umDTkEql2NzczHqtTYAxOd/InwwMDISqURMiKEhC7Lu9sLAAQDKZzDqeLwV/or6+/i/VZaMJEUSSkNPTUwCOjo4Af++QSqV+e87w8DAADQ0NAIyMjADQ3t6e89ypqSkAqqqqQlKdQRMiCHUfcnBwAEBPTw8A19fXOV/X1tYGQFNTEwCJRILx8XEASkpKAKitrQXg6uoq57kbGxsAlJeXv0Tir+g+JAih1hD7aVFUlPE5kUgAMDc3B0BlZSUAdXV1AFRUVDy7RjqdBvyaYa9ZWloKwMzMDPBXyfgjkWzdz87OAGhsbAx8jjWit7cXgO3tbcAvqtaI7u7u10jKhd4yQYhFcwcwODgI+O18Z2cnAOvr60Akt4gmJAixSMja2hpDQ0OA3+7f3t4C0RVPNCHBcJqQ/f19ILORs8moqakB4PLyMsqlQRMSDKdD5unpaSBTN2wydnd3XUrShEgKmpCHhwfAH/utrKwAmSZvayvzm6Hq6upCSnqGJkRQ0IQcHx8DsLq6CvhNYDKZdJ4MiyZEUJCETE5OArC4uJh13I4N7XAoDmhCBJHuVO2wuaurC3g+Ujw5OQFeNjcJEd2pBiGShNhrFhdnl6iWlhYADg8PX3PZsNGEBCGST5n5+XnAHxDbPsV+6SXOaEIEodaQu7vMr0w7OjoAf/q+t7cHQGtr68sVRofWkCCEWkMeHx8BPxn9/f0ANDc3h7lMpGhCBLGYujtCa0gQ8tWQ13/X6R9FEyJQQwRqiEANEaghAjVE8B07pyBEuWWOHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = show_at(train_ds, 3, cmap=\"Greys\", figsize=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ax.title.get_text() in ('3','7')\n",
    "test_fig_exists(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToTensor -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToTensor(Transform):\n",
    "    \"Convert item to appropriate tensor class\"\n",
    "    order = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@docs\n",
    "class Cuda(Transform):\n",
    "    \"Move batch to `device` (defaults to `default_device()`)\"\n",
    "    def __init__(self,device=None):\n",
    "        self.device=default_device() if device is None else device\n",
    "        super().__init__(split_idx=None, as_item=False)\n",
    "    def encodes(self, b): return to_device(b, self.device)\n",
    "    def decodes(self, b): return to_cpu(b)\n",
    "\n",
    "    _docs=dict(encodes=\"Move batch to `device`\", decodes=\"Return batch to CPU\")\n",
    "    \n",
    "    #Note: in pipeline or batch_tfms (for after_batch) in DataSource, you can just put in 'to_device' partial function, \n",
    "    # instead of using a separate transform class like this\n",
    "    # but you lose the 'decode' part. Your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, like all `Transform`s, `encodes` is called by `tfm()` and `decodes` is called by `tfm.decode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cuda()(tensor(1)) # again, Transform class can be performed on single item as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1, device='cuda:0'),)\n"
     ]
    }
   ],
   "source": [
    "tfm = Cuda()\n",
    "t = tfm((tensor(1),))\n",
    "print(t)\n",
    "test_eq(*t,1)\n",
    "test_eq(t[0].type(),'torch.cuda.LongTensor' if default_device().type=='cuda' else 'torch.LongTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tfm.decode(t)\n",
    "test_eq(*t,1)\n",
    "test_eq(t[0].type(),'torch.LongTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(Transform): \n",
    "    def encodes(self, x): return x \n",
    "    def decodes(self, x): return Int(x) \n",
    "    \n",
    "start = torch.arange(0,50)\n",
    "tds = DataSource(start, [A()])\n",
    "tdl = TfmdDL(tds, after_batch=Cuda, bs=4)\n",
    "test_eq(tdl.device, default_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IntToFloatTensor, as_item and type reserve (part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class IntToFloatTensor(Transform):\n",
    "    \"Transform image to float tensor, optionally dividing by 255 (e.g. for images).\"\n",
    "    order = 20 #Need to run after CUDA if on the GPU\n",
    "    def __init__(self, div=255., div_mask=1, split_idx=None, as_item=True):\n",
    "        super().__init__(split_idx=split_idx,as_item=as_item)\n",
    "        self.div,self.div_mask = div,div_mask\n",
    "\n",
    "    def encodes(self, o:TensorImage): return o.float().div_(self.div)\n",
    "    def encodes(self, o:TensorMask ): return o.div_(self.div_mask).long() # convert mask to long int\n",
    "    def decodes(self, o:TensorImage): return o.clamp(0., 1.) if self.div else o\n",
    "    # Important: if o does not match with any type (TensorImage or TensorMask) => no encode is performed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TensorImage(tensor(1)) # note that similarly to all transform above, this transform can be executed on 1 single things (doesnt have to be a tuple)\n",
    "tfm = IntToFloatTensor(as_item=False)\n",
    "ft = tfm(t)\n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1), tensor(1))\n",
      "tensor(0.0039)\n"
     ]
    }
   ],
   "source": [
    "t = (TensorImage(tensor(1)),TensorImage(tensor(1)))\n",
    "tfm = IntToFloatTensor(as_item=True) # note that if you input a Tuple type and as_item=True, the whole Tuple is treated as one single item\n",
    "# and since there is no specific encode for tuple type, no transformation is performed.\n",
    "ft = tfm(t)\n",
    "print(ft)\n",
    "\n",
    "\n",
    "t = TensorImage(tensor(1))\n",
    "tfm = IntToFloatTensor(as_item=True) # with only one thing (not a tuple), and as_item=True, the type is recognized, so tfm is performed\n",
    "ft = tfm(t)\n",
    "print(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (TensorImage(tensor(1)),tensor(2).long(),TensorMask(tensor(3)))\n",
    "tfm = IntToFloatTensor(as_item=False)\n",
    "ft = tfm(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0039), tensor(2), tensor(3))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft # TensorImage got floated and div 255, ordinary tensor not transformed, TensorMask got div by 1 so nothing happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm.decode(ft[0]) # TODO: isn't decode suppose to switch this back to 255 scale? Why clamp (0,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai2.torch_core.TensorImage'> torch.LongTensor\n",
      "<class 'fastai2.torch_core.TensorImage'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "#input type vs output type. Notice that fastai type is reserved (the custom fastai type - TensorImage, not the pytorch type, due to IntToFloat tfms)\n",
    "print(type(t[0]),t[0].type()) #input\n",
    "print(type(ft[0]),ft[0].type()) #output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reserve type for decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai2.torch_core.TensorImage'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "print(type(tfm.decode(ft[0])), tfm.decode(ft[0]).type()) #decode reserves both the pytorch type and fastai type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ft, [1./255, 2, 3])\n",
    "test_eq(type(ft[0]), TensorImage)\n",
    "test_eq(type(ft[2]), TensorMask)\n",
    "test_eq(ft[0].type(),'torch.FloatTensor')\n",
    "test_eq(ft[1].type(),'torch.LongTensor')\n",
    "test_eq(ft[2].type(),'torch.LongTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick review on parameters passing in function and .view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_func(a,b,*c,**d):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "(3, 4, 5)\n",
      "{'fuck': '6'}\n"
     ]
    }
   ],
   "source": [
    "temp_func(1,2,3,4,5,fuck=\"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor([1,2,3,4,5,6,7,8]).view(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor([1,2,3,4,5,6,7,8]).view(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor([1,2,3,4,5,6,7,8]).view(4,2).view(-1) # straighten back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def broadcast_vec(dim, ndim, *t, cuda=True):\n",
    "    \"Make a vector broadcastable (change to correct shape for matrix calculation) over `dim` (out of `ndim` total) for EACh value in t by prepending and appending unit axes\"\n",
    "    v = [1]*ndim\n",
    "    v[dim] = -1\n",
    "    f = to_device if cuda else noop\n",
    "    return [f(tensor(o).view(*v)) for o in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[3]]], device='cuda:0') torch.Size([3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "temp = [1,2,3]\n",
    "temp1 = broadcast_vec(0,3,temp)[0] \n",
    "print(temp1,temp1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1]],\n",
       "\n",
       "        [[2]],\n",
       "\n",
       "        [[3]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(temp).view(-1,1,1) # which is just tensor(temp).view(3,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1]],\n",
      "\n",
      "         [[2]],\n",
      "\n",
      "         [[3]]]], device='cuda:0') torch.Size([1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "temp1 = broadcast_vec(1,4,temp)[0] # you can even add new axis\n",
    "print(temp1,temp1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1., -1.],\n",
       "          [-1., -1.]],\n",
       "\n",
       "         [[-2., -2.],\n",
       "          [-2., -2.]],\n",
       "\n",
       "         [[-3., -3.],\n",
       "          [-3., -3.]]],\n",
       "\n",
       "\n",
       "        [[[-1., -1.],\n",
       "          [-1., -1.]],\n",
       "\n",
       "         [[-2., -2.],\n",
       "          [-2., -2.]],\n",
       "\n",
       "         [[-3., -3.],\n",
       "          [-3., -3.]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need this broadcast func to turn mean (or std) vector in to broadcastable vector, in order to do this\n",
    "temp1 = broadcast_vec(1,4,temp,cuda=False)[0] \n",
    "torch.zeros(2,3,2,2) - temp1 # image_batch - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@docs\n",
    "class Normalize(Transform):\n",
    "    \"Normalize/denorm batch of `TensorImage`\"\n",
    "    order=99\n",
    "    def __init__(self, mean=None, std=None, axes=(0,2,3)): self.mean,self.std,self.axes = mean,std,axes\n",
    "    \n",
    "    @classmethod\n",
    "    def from_stats(cls, mean, std, dim=1, ndim=4, cuda=True): return cls(*broadcast_vec(dim, ndim, mean, std, cuda=cuda))\n",
    "    \n",
    "    def setups(self, dl:DataLoader): \n",
    "        if self.mean is None or self.std is None: # if no mean or std is provided, calculate meand and std from 1 batch\n",
    "            x,*_ = dl.one_batch() \n",
    "            self.mean,self.std = x.mean(self.axes, keepdim=True),x.std(self.axes, keepdim=True)+1e-7\n",
    "\n",
    "    def encodes(self, x:TensorImage): return (x-self.mean) / self.std\n",
    "    def decodes(self, x:TensorImage):\n",
    "        f = to_cpu if x.device.type=='cpu' else noop\n",
    "        return (x*f(self.std) + f(self.mean))\n",
    "\n",
    "    _docs=dict(encodes=\"Normalize batch\", decodes=\"Denormalize batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[open_img, img2tensor],\n",
    "        [parent_label, Categorize()]]\n",
    "train_ds = DataSource(train, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean,std = [0.5]*3,[0.5]*3\n",
    "mean,std = broadcast_vec(1, 4, mean, std)\n",
    "batch_tfms = [Cuda(), IntToFloatTensor(), Normalize.from_stats(mean,std)]\n",
    "tdl = TfmdDL(train_ds, after_batch=batch_tfms, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y  = tdl.one_batch()\n",
    "xd,yd = tdl.after_batch.decode((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(x.type(), 'torch.cuda.FloatTensor' if default_device().type=='cuda' else 'torch.FloatTensor')\n",
    "test_eq(xd.type(), 'torch.FloatTensor')\n",
    "test_eq(type(x), TensorImage)\n",
    "test_eq(type(y), TensorCategory)\n",
    "assert x.mean()<0.0\n",
    "assert x.std()>0.5\n",
    "assert 0<xd.mean()/255.<1\n",
    "assert 0<xd.std()/255.<0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai2.torch_core.TensorImage'> torch.ByteTensor\n",
      "<class 'fastai2.torch_core.TensorImage'> torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# another example of keeping 'decode' input and output the same\n",
    "print(type(train_ds[0][0]),train_ds[0][0].type()) # input x\n",
    "print(type(x),x.type()) # output x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai2.torch_core.TensorCategory'> torch.LongTensor\n",
      "<class 'fastai2.torch_core.TensorCategory'> torch.cuda.LongTensor\n"
     ]
    }
   ],
   "source": [
    "print(type(train_ds[0][1]),train_ds[0][1].type()) # input y\n",
    "print(type(y),y.type()) # output y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai2.torch_core.TensorImage'> torch.FloatTensor\n",
      "<class 'fastai2.torch_core.TensorCategory'> torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "print(type(xd),xd.type()) # decode output for output x\n",
    "print(type(yd),yd.type()) # decode output for output y\n",
    "\n",
    "# throughout all this, normal type (fastai type) is reserved\n",
    "# but you can see pytorch type change, e.g. for x it goes from ByteTensor to cuda FloatTensor (Cuda + IntToFloat tfms) to FloatTensor\n",
    "# and for y it goes from LongTensor to cuda LongTensor to LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for visuals\n",
    "from fastai2.vision.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdl.show_batch((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai2.torch_core.TensorImage'> torch.cuda.FloatTensor\n",
      "<class 'fastai2.torch_core.TensorCategory'> torch.cuda.LongTensor\n"
     ]
    }
   ],
   "source": [
    "print(type(x),x.type())\n",
    "print(type(y),y.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.cuda.FloatTensor\n",
      "<class 'torch.Tensor'> torch.cuda.LongTensor\n"
     ]
    }
   ],
   "source": [
    "x,y = torch.add(x,0),torch.add(y,0) #Lose type of tensors (to emulate predictions)\n",
    "print(type(x),x.type())\n",
    "print(type(y),y.type()) # normal type is changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ne(type(x), TensorImage)\n",
    "tdl.show_batch((x,y), figsize=(4,4)) #x and y types are put back to their original: TensorImage and TensorCategory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
